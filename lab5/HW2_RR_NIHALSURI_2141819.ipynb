{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RR_e2aPI2d7g"
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\" and remove every line containing the expression: \"raise ...\" (if you leave such a line your code will not run).\n",
    "\n",
    "Do not remove any cell from the notebook you downloaded. You can add any number of cells (and remove them if not more necessary).\n",
    "\n",
    "Do not leave any variable initialized to None.\n",
    "\n",
    "## IMPORTANT: make sure to rerun all the code from the beginning to obtain the results for the final version of your notebook, since this is the way we will do it before evaluating your notebook!!!\n",
    "\n",
    "## Make sure to name your notebook file (.ipynb) correctly:\n",
    "### - HW2_RR_NAMESURNAME_ID (E.g. : HW2_RR_MARIOROSSI_2204567)\n",
    "\n",
    "## Fill in your name, surname and id number (numero matricola) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5qSPHtp2d7h"
   },
   "outputs": [],
   "source": [
    "NAME = \"NIHAL SURI\"\n",
    "ID_number = int(\"2141819\")\n",
    "\n",
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLkP9bIQ2d7i"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oxg8wRM92d7i",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c422ae78203462a9498cca8944c92663",
     "grade": false,
     "grade_id": "cell-f16f58e882e53ce1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#  Regression on House Pricing Dataset: Variable Selection & Regularization\n",
    "\n",
    "We will consider a reduced version of a dataset containing house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n",
    "\n",
    "Dataset used:\n",
    "https://www.kaggle.com/harlfoxem/housesalesprediction\n",
    "\n",
    "{\n",
    "Kaggle competition on house prices:\n",
    "https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "}\n",
    "\n",
    "For each house we know 17 house features (e.g., number of bedrooms, number of bathrooms, etc.) plus its price, that is what we would like to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0AxC3Xvw2d7j",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "500681e1005aa61e8f0a7756a5d1b80d",
     "grade": false,
     "grade_id": "cell-d3dd2a8ca67a5550",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import all packages needed\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(ID_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "deletable": false,
    "editable": false,
    "id": "Z1pQtpmj2d7j",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efb7499a108eee0c5c2b50cbe9f0ca22",
     "grade": false,
     "grade_id": "cell-097b67c95fb49e02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e5e90a89-cabf-4ec9-e11c-2773edc6283c"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/LucaZancato/ML2020-2021/main/kc_house_data.csv\"\n",
    "\n",
    "data = pd.read_csv(url, sep=',')\n",
    "\n",
    "# Remove the data samples with missing values (NaN)\n",
    "data = data.dropna()\n",
    "# Remove the houses with waterfront\n",
    "data = data.drop(data[data['waterfront']==1].index)\n",
    "# Remove the columns we are not going to use\n",
    "data = data.drop(columns=['id', 'date','waterfront'])\n",
    "# Have a brief description of the dataset\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deletable": false,
    "editable": false,
    "id": "QYs8G95k2d7k",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "255306146d2640ef9b32ae530fd824a8",
     "grade": false,
     "grade_id": "cell-b6daf09836b5e747",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "167b7ae7-8fef-4744-def3-abd8b3c1a82f"
   },
   "outputs": [],
   "source": [
    "# Print first 5 datapoints of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "ryGOLZlW2d7k",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f375eb4e8ee3f608963dcd357b1634a",
     "grade": false,
     "grade_id": "cell-19d9f9b20f9e4c97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "b2f0cb11-73f6-49b8-ea18-65d5dc4f03db"
   },
   "outputs": [],
   "source": [
    "# Let's look at all the possible independent variables and get an idea of our data. Do not forget we are going\n",
    "# to predict the variable 'price' using all the other features\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "f_UFW0O82d7l",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cec47b5990ec53528596280ad98347c",
     "grade": false,
     "grade_id": "cell-2a5b099a83b89ca7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's split data into train and test using sklearn built-in function: train_test_split (have a look at the\n",
    "# documentation)\n",
    "m_t, m = 100, len(data)\n",
    "m_test = m - m_t\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data, test_size=m_test/m, random_state=ID_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "deletable": false,
    "editable": false,
    "id": "iPRQzC-S2d7l",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9bb4c9671e0ea7bef0f16014ded6921",
     "grade": false,
     "grade_id": "cell-b8b95b0df44b8c41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "727d3b3c-a64a-47f1-be71-9295c9cef4ad"
   },
   "outputs": [],
   "source": [
    "# Let's check the price trend as a function of the sqrt_living and sqrt_basement (separately)\n",
    "\n",
    "def plot_single_feature_vs_y(feature, train_data, ax=None, y='price'):\n",
    "    reduced_data = pd.concat([train_data[y], train_data[feature]], axis=1)\n",
    "    reduced_data.plot.scatter(x=feature, y=y, ylim=(train_data[y].min(), train_data[y].max()), ax=ax)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "plot_single_feature_vs_y('sqft_living', train_data, ax=axes[0])\n",
    "plot_single_feature_vs_y('sqft_basement', train_data, ax=axes[1])\n",
    "\n",
    "# Note 'sqft_basement' might not be easily used to predict Y (many values are zero while 'price' has different values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WQBvxsXU2d7m",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5d737ad847c5ca24b565d4376afcb67",
     "grade": false,
     "grade_id": "cell-11fe19b6f302f4b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's explore data and features distributions using box plots\n",
    "def box_plot_single_feature_vs_y(feature, train_data):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    reduced_data = pd.concat([train_data['price'], train_data[feature]], axis=1)\n",
    "    sns.boxplot(x=feature, y=\"price\", data=reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 874
    },
    "deletable": false,
    "editable": false,
    "id": "6tfNWOBn2d7n",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f9a438faed88adfbe72b2132a2a3b09",
     "grade": false,
     "grade_id": "cell-449c873a0c31688d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "0450ef46-f6f9-4dc7-893c-4e60a7449601"
   },
   "outputs": [],
   "source": [
    "# Prices over 'grade'\n",
    "box_plot_single_feature_vs_y('grade', train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "deletable": false,
    "editable": false,
    "id": "YuKs8GsA2d7n",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5e68ceea47c0aec97d88db1b80b701b",
     "grade": false,
     "grade_id": "cell-e7613d677e909ec0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "0f0ac77d-0191-4724-8247-46a13104c2ca"
   },
   "outputs": [],
   "source": [
    "# Prices over 'yr_built'\n",
    "box_plot_single_feature_vs_y('yr_built', train_data)\n",
    "plt.xticks(rotation=55)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3Oxm5xDG2d7n",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "705bb2507594942bb827180fbfd59700",
     "grade": false,
     "grade_id": "cell-325588abe2699334",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Note**: 'grade' seems to correlate well with the regression variable 'price' (the higher the 'grade' the higher the\n",
    " 'price'). On the other hand it is not clear whether there is correlation between 'yr_built' and 'price'.\n",
    " You can try to inspect other features and how they correlate with 'price'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_4VtLpP2d7n"
   },
   "source": [
    "Additional Box Plots of parameters that correllate well with the price other than **grade**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF1HaW4k2d7n"
   },
   "outputs": [],
   "source": [
    "# box_plot_single_feature_vs_y('bathrooms', train_data)\n",
    "# box_plot_single_feature_vs_y('view', train_data)\n",
    "# box_plot_single_feature_vs_y('sqft_living15', train_data)\n",
    "# plt.xticks(rotation=70)\n",
    "# box_plot_single_feature_vs_y('sqft_above', train_data)\n",
    "# plt.xticks(rotation=70)\n",
    "# box_plot_single_feature_vs_y('sqft_living', train_data)\n",
    "# plt.xticks(rotation=70)\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 939
    },
    "deletable": false,
    "editable": false,
    "id": "jTGzGBoX2d7o",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d959d858f4dcb78b9584d15a5b176840",
     "grade": false,
     "grade_id": "cell-0ac371719a0c682c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c0285cd3-ba72-4117-f666-383f8cedf69f"
   },
   "outputs": [],
   "source": [
    "# Let's' try to make the process we followed up to now a little bit more systematic: we will use a pandas\n",
    "# built-in function to plot the correlation matrix between all the features and the regression variables.\n",
    "\n",
    "def plot_correlation_matrix(df_train):\n",
    "    plt.subplots(figsize=(15, 10))\n",
    "    corr_matrix = df_train.corr()\n",
    "    ax = sns.heatmap(corr_matrix, vmax=1, square=False)\n",
    "    ax.set_ylim(19, 0)\n",
    "    return corr_matrix\n",
    "\n",
    "corr_matrix = plot_correlation_matrix(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "deletable": false,
    "editable": false,
    "id": "DIxM43lM2d7o",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac3dba52bb6ca788462ee620cb505589",
     "grade": false,
     "grade_id": "cell-6ffb5ec1800d8c30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "a7831318-5f48-43f6-a976-4fcba93eaa14"
   },
   "outputs": [],
   "source": [
    "# Previous correlation matrix is not ordered, we need to sort its entries such that we can cluster the most\n",
    "# correlated variables. In this way it will be easier to read the correlation matrix.\n",
    "\n",
    "import scipy\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "def cluster_corr(corr_array, inplace=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rearranges the correlation matrix, corr_array, so that groups of highly\n",
    "    correlated variables are next to eachother\n",
    "\n",
    "    :param corr_array: pandas.DataFrame or numpy.ndarray a NxN correlation matrix\n",
    "\n",
    "    :returns: A NxN correlation matrix with the columns and rows rearranged\n",
    "    \"\"\"\n",
    "    pairwise_distances = sch.distance.pdist(corr_array)\n",
    "    linkage = sch.linkage(pairwise_distances, method='complete')\n",
    "    cluster_distance_threshold = pairwise_distances.max()/2\n",
    "    idx_to_cluster_array = sch.fcluster(linkage, cluster_distance_threshold,\n",
    "                                        criterion='distance')\n",
    "    idx = np.argsort(idx_to_cluster_array)\n",
    "\n",
    "    if not inplace:\n",
    "        corr_array = corr_array.copy()\n",
    "\n",
    "    if isinstance(corr_array, pd.DataFrame):\n",
    "        return corr_array.iloc[idx, :].T.iloc[idx, :]\n",
    "    return corr_array[idx, :][:, idx]\n",
    "\n",
    "plt.subplots(figsize=(15, 10))\n",
    "corr_matrix = cluster_corr(corr_matrix, inplace=False)\n",
    "ax = sns.heatmap(corr_matrix, vmax=1, square=False)\n",
    "ax.set_ylim(19, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DyfG77s62d7o",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "316994827eb8218bbf3b078cda6fab7e",
     "grade": false,
     "grade_id": "cell-aab46ed7081a9550",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note the 'clusters' along the diagonal. Actually, some of these correlations are so strong that it might indicate a situation of multicollinearity.\n",
    "This means these variables are in some sense redundant (they give almost the same information) and might not be useful to build our final linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": false,
    "editable": false,
    "id": "WR3Dsp5r2d7p",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29c86bc7ae0b366dec449691ce6b6b45",
     "grade": false,
     "grade_id": "cell-6282d683a72b8582",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "ed297b18-488b-4d90-c333-a52b96b0dcba"
   },
   "outputs": [],
   "source": [
    "# Let's have a look at some scatter plots (in the main diagonal there is a histogram with the actual data)\n",
    "sns.set()\n",
    "cols = ['price', 'bathrooms', 'sqft_living', 'condition', 'sqft_basement', 'grade', 'yr_built']\n",
    "sns.pairplot(train_data[cols], size = 2.5)\n",
    "plt.show()\n",
    "# As we see in the correlation matrix 'condition' and 'yr_build' are very poorly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5I44sApb2d7p",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ff83b9249032cdfead2a2f9cfe1b429",
     "grade": false,
     "grade_id": "cell-044f6ee32d15ecc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Note**: We can remove features that we believe are much correlated with others (leaving at least one of them), since they can be thought as redundant. Don't forget we should keep into account how much each feature is correlated with the regression variable too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NCHXqjeR2d7p",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4746392387d0673a71e5f40e32bf4a20",
     "grade": false,
     "grade_id": "cell-e78d57b29d9b8852",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's now standardize the data as we did in the past homeworks\n",
    "features_names = train_data.columns[1:]\n",
    "x_train, y_train = train_data[features_names].values.astype('float'), train_data['price'].values.astype('float')\n",
    "x_test, y_test = test_data[features_names].values.astype('float'), test_data['price'].values.astype('float')\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler_x = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = scaler_x.transform(x_train)\n",
    "\n",
    "scaler_y = preprocessing.StandardScaler().fit(y_train.reshape(-1,1))\n",
    "y_train = scaler_y.transform(y_train.reshape(-1,1)).reshape(-1,)\n",
    "\n",
    "x_test = scaler_x.transform(x_test)\n",
    "y_test = scaler_y.transform(y_test.reshape(-1,1)).reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtzjqmPz2d7p"
   },
   "source": [
    "**Additional block:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ccc00KAn2d7p",
    "outputId": "719bf5df-06a3-4b3e-83dc-49d08acc6769"
   },
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "d = x_train.shape[1]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BGXIRYF2d7q"
   },
   "source": [
    "**Additional block:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vRXqd_072d7q",
    "outputId": "2bbebd49-67c7-46f5-e6c1-df83b4058de9"
   },
   "outputs": [],
   "source": [
    "print(f\"Mean of the training input data:   {x_train.mean(axis=0)}\")\n",
    "print(f\"Std of the training input data:    {x_train.std(axis=0)}\")\n",
    "print(f\"Mean of the test input data:   {x_train.mean(axis=0)}\")\n",
    "print(f\"Std of the test input data:    {x_train.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmO0yz3U2d7q"
   },
   "source": [
    "**Additional function to check bias of a set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vHoXtB12d7q"
   },
   "outputs": [],
   "source": [
    "def add_bias_column(matrix):\n",
    "    # Ensure matrix is 2-dimensional\n",
    "    matrix = matrix.reshape(-1, 1) if matrix.ndim == 1 else matrix\n",
    "    # Add bias column only if it's not already present\n",
    "    if not np.all(matrix[:, 0] == 1):\n",
    "        matrix = np.hstack((np.ones((matrix.shape[0], 1)), matrix))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "D0oVG6oB2d7q",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "513953a561896d16528893c59013f77d",
     "grade": false,
     "grade_id": "cell-66e4de9bb3e06905",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 1\n",
    "# Write a function to compute the Least-Squares estimate using LinearRegression() from Scikit-learn given x_train and\n",
    "# y_train. The function must return the COD both for training and test dataset AND must return a vector containing\n",
    "# all the model parameters (both bias b and coefficients w)\n",
    "from sklearn import linear_model\n",
    "def solve_LS_problem(x_train : np.ndarray, y_train : np.array, x_test : np.ndarray, y_test : np.array) -> tuple:\n",
    "    '''\n",
    "    Funtion used to compute the LS estimate given train data. This function uses Scikit-learn to get both the LS\n",
    "    solution and other required quantities.\n",
    "    :param x_train: input data used to get the linear model predictions\n",
    "    :param y_train: output data to be predicted\n",
    "    :param x_test: test features used to assess model performance\n",
    "    :param y_test: test output to be predicted to assess model performance\n",
    "\n",
    "    :returns: (COD_train, COD_test, w)\n",
    "        WHERE\n",
    "        COD_train : Coefficient of determination for the training dataset (float)\n",
    "        COD_test : Coefficient of determination for the test dataset (float)\n",
    "        w : parameters of the linear model (the bias is contained, return it as the first element of w)\n",
    "            of shape (#parameters + 1,)\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # # to make sure that the size is the original one, before stacking with ones\n",
    "    # assert x_train.shape[1] == d\n",
    "    # assert x_test.shape[1] == d\n",
    "\n",
    "    # stack data with ones, so as to add bias term\n",
    "    # x_train = add_bias_column(x_train)\n",
    "    # x_test  = add_bias_column(x_test)\n",
    "\n",
    "    # train the model\n",
    "    reg = linear_model.LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "    # calculate CODs\n",
    "    COD_train = np.float64(reg.score(x_train, y_train))\n",
    "    COD_test = np.float64( reg.score(x_test, y_test))\n",
    "\n",
    "    # concatenate intercept with coefficients\n",
    "    w = np.insert(reg.coef_, 0, reg.intercept_)\n",
    "\n",
    "    return (COD_train, COD_test, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "fVgtJ9ms2d7q",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "099c814a55fade592074958060cd772b",
     "grade": true,
     "grade_id": "cell-fb78af2ff311762d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "60d9327a-1ec5-42ea-d4b8-04ea5ace08f5"
   },
   "outputs": [],
   "source": [
    "COD_train_LS_full, COD_test_LS_full, w_LS_full = solve_LS_problem(x_train, y_train, x_test, y_test)\n",
    "print(f\"Coefficient of determination on training data: {COD_train_LS_full:.4f}\")\n",
    "print(f\"Coefficient of determination on test data:     {COD_test_LS_full:.4f}\")\n",
    "print(f\"w_LS_full:{w_LS_full}\")\n",
    "\n",
    "assert w_LS_full.shape == (18,)\n",
    "assert type(COD_train_LS_full) == np.float64 and COD_train_LS_full <= 1.0\n",
    "assert type(COD_test_LS_full) == np.float64 and COD_test_LS_full <= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IcCKcF9-2d7q",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f28db29bb3fd83fe596820de5e38423",
     "grade": false,
     "grade_id": "cell-2235169744067cfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TODO 2: explain the results\n",
    "Explain the effect of the pre-processing on the first value of w_LS_full."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "hD88rTcE2d7r",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf93b192b6614dc97e43cdefc031ab79",
     "grade": true,
     "grade_id": "cell-6f5fd3a5954c29d1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Similar to how we had seen in **HW_1_LR** when preprocessing is done what we're doing in practice is scaling our data so that the **mean** and **std** (the variance does not affect this case here, its only the mean but its worth mentioning) are 0 and 1 respectively. The **first value of w_LS_full** represents the **intercept**, so in the following formula for a linear regression model: $$h(x) = w^Tx + b$$ It is the $b$ value. $b$ represents $\\mathbb{E}[h(x)]$ when all x is equal to 0, and assuming that we have gaussian noise. Now in the earlier step we have centered our data, so the point $(x=0, y=0)$ is exactly the center of the data and the regression hyperplane passes through the origin ensuring the intercept is also 0, and the regression equation is nothing but the following in the end $$h(x) = w^Tx$$. When we calculate the Least-Squares estimate using LinearRegression() from Scikit-learn, the intercept is not exactly 0 but a value that is extremely close to it, in our case it is $2.79e^{-15}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "6BmDWzmY2d7r",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4957e3882ea07b3276a1cc719008f72",
     "grade": false,
     "grade_id": "cell-e8d945e00dd945be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "abee8420-d5c6-4959-c7ff-3d2e097fc47f"
   },
   "outputs": [],
   "source": [
    "# TODO 3\n",
    "# Based on the observations we made earlier looking at the dataset (correlation and scatter plots) which variables\n",
    "# would you choose to predict the price? Choose the 4 most important features based on your intuition.\n",
    "# Here we plot features and their indeces for your ease of use\n",
    "print({index: feature for index, feature in enumerate(features_names)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "oSkOH4Fn2d7r",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd8d64ab5faa9f9ba548a76e736a014d",
     "grade": false,
     "grade_id": "cell-6a4f18cc63ce1c3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "e86733c3-d461-4963-b69a-f139a586896a"
   },
   "outputs": [],
   "source": [
    "hand_selected_features_indeces = None # Replace with a list of 4 indeces, select them from the train and test sets and then solve the reduced (in the number of\n",
    "                                      # features) LS problem using the function we built before\n",
    "COD_train_LS_reduced, COD_test_LS_reduced, w_LS_reduced = None, None, None  # Replace with proper values\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# # the features that are choosen are grade, sqft_living, condition, bathrooms\n",
    "# hand_selected_features_indeces = [7, 2, 6, 1]\n",
    "\n",
    "# # the features that are choosen are grade, sqft_living, condition, bathrooms\n",
    "# hand_selected_features_indeces = [7, 2, 6, 1]\n",
    "\n",
    "# # the features that are choosen are grade, sqft_living, lat, bathrooms\n",
    "hand_selected_features_indeces = [7, 2, 13, 1]\n",
    "\n",
    "# # from BSS\n",
    "# hand_selected_features_indeces = [2, 7, 13, 10]\n",
    "\n",
    "# solve the reduced LS problem\n",
    "COD_train_LS_reduced, COD_test_LS_reduced, w_LS_reduced = solve_LS_problem(x_train[:, hand_selected_features_indeces], y_train,\n",
    "                                                                  x_test[: , hand_selected_features_indeces], y_test)\n",
    "\n",
    "print(f\"Coefficient of determination on training data: {COD_train_LS_reduced:.4f}\")\n",
    "print(f\"Coefficient of determination on test data:     {COD_test_LS_reduced:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMx5H5D02d7s",
    "outputId": "47c6564f-fafb-4466-9895-61a32a9a393f"
   },
   "outputs": [],
   "source": [
    "# Additional\n",
    "print(w_LS_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yXyGwDor2d7s",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25d9403c08943ce4bcc0a9bad1e4500b",
     "grade": true,
     "grade_id": "cell-0a3c880c0c358be1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert w_LS_reduced.shape == (5,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "oSsJzdB22d7s",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b812fc64aeec178a989eef6de9c10bcc",
     "grade": false,
     "grade_id": "cell-cd8376321e29bd1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TODO 4:\n",
    "What is your reasoning behind the choice of the 4 features you selected in the todo 3?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "FxsiToht2d7s",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7a0b3472e21e29e122193d0bb3037f4",
     "grade": true,
     "grade_id": "cell-d49d4e95b93186a7",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The reason behind this choice was running an qualitative analysis on the following: \n",
    "\n",
    "1. **Box Plots:** After plotting additional box plots to visualize which features correlate best with 'price' other than grade which we're already aware about. From this observation I came to the conclusion that the following features correllate decently well with 'price':\n",
    "    - grade\n",
    "    - sqft_living\n",
    "    - sqft_living15\n",
    "    - bathrooms\n",
    "    - sqft_above\n",
    "    - bedrooms\n",
    "    - lat\n",
    "\n",
    "2. **Ordered Correlation Matrix:** This provides more intution into the features that were selected at the first stage using the box plots, in the sense to show multicollinearity between themselves or not. For example, from this we could understand that **'sqft_living', 'sqft_above' and 'sqft_living15' are highly correlated** so even though they correllate with price well, choosing all these features would introduce multicollinearity, and in the end we would like to improve the diversity of the model. It is also the similar situation with **'bathrooms' and 'bedrooms'** both are highly correllated. So we choose one of them which has the strongest correlation with 'price' and exclude the rest, in the current case that is **bathrooms**. **lat** is a good choice for a feature as well, because its correlated fairly well with **price**, but also what is important to note is that **it has almost no correlation with other selected features** and therefore we don't have to concern ourselves with multicollinearity for **lat**. \n",
    "\n",
    "3. **Scatter Plots correllation with features:** This visualization technique further enhances our ability to spot multicorrelation between features, as each datapoint is marked here and is not provided as only a heatmap on the correaltion matrix. It is easier to spot multicollinearity between two features, and verify the selection from the correlation matrix. \n",
    "\n",
    "The visualizations used to make this decesion can be viewed below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArRbYwPg2d7t"
   },
   "source": [
    "**Addition scatter plot to spot multicorrelation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t95boViE2d7t"
   },
   "outputs": [],
   "source": [
    "### UNCOMMENT BEFORE SUBMISSION\n",
    "cols = [ 'price', 'sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_above', 'bedrooms', 'bathrooms', 'grade']\n",
    "sns.pairplot(train_data[cols], size = 2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNv89mNy2d7t"
   },
   "source": [
    "**Additional box plots to show correlation of certain features with 'price'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wlm3egv82d7t"
   },
   "outputs": [],
   "source": [
    "### UNCOMMENT BEFORE SUBMISSION\n",
    "box_plot_single_feature_vs_y('bathrooms', train_data)\n",
    "box_plot_single_feature_vs_y('lat', train_data)\n",
    "plt.xticks(rotation=80)\n",
    "box_plot_single_feature_vs_y('bedrooms', train_data)\n",
    "box_plot_single_feature_vs_y('condition', train_data)\n",
    "plt.xticks(rotation=80)\n",
    "box_plot_single_feature_vs_y('sqft_living15', train_data)\n",
    "plt.xticks(rotation=70)\n",
    "box_plot_single_feature_vs_y('sqft_above', train_data)\n",
    "plt.xticks(rotation=70)\n",
    "box_plot_single_feature_vs_y('sqft_living', train_data)\n",
    "plt.xticks(rotation=70)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "HmOSH22Y2d7t",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a7f4f0df75727f366a7d4bf554958d0",
     "grade": false,
     "grade_id": "cell-131d49e1216d7388",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Best-Subset Selection\n",
    "\n",
    "What if we try a brute force approach? Are the features you selected the same as the ones you would get by looking at all the possible combinations?\n",
    "\n",
    "In the next cell we are going to split x_train into a (\"new\") training dataset (x_train_BSS) and validation dataset (x_val_BSS) to perform best-subset selection (remember the validation dataset is used to find the best generalizing model among the ones, with different number of features, you trained using x_train_BSS).\n",
    "\n",
    "We are going to choose subsets of features going from $1$ to $n_{sub}=4$. In theory we should try all the possible combinations of size $1,2, \\dots, 18$ but the number of models to train and validate would be huge! For the sake of simplicity we will choose all the possible subsets of dimension $1$ to $4$.\n",
    "\n",
    "Steps:\n",
    "1. Compute the LS estimate using all the possible subsets of $k$ features\n",
    "2. Compute the prediction error on the validation dataset\n",
    "3. Choose the subset of $k^*$ features giving the lowest validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "bRa8dt1g2d7t",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83493c30696448304390f42b171a0daf",
     "grade": false,
     "grade_id": "cell-d8c459868acb52da",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "245fb707-f9d0-4c45-8802-4bf8db956368"
   },
   "outputs": [],
   "source": [
    "# TODO 5\n",
    "# Let's get the validation dataset from the training dataset (don't forget they must be disjoint, of course we could\n",
    "# have splitted them before, during the pre-processing step)\n",
    "import itertools\n",
    "\n",
    "m_train_BSS, m_val_BSS = m_t // 2, m_t - m_t // 2\n",
    "\n",
    "x_train_BSS, y_train_BSS = x_train[:m_train_BSS], y_train[:m_train_BSS]\n",
    "x_val_BSS, y_val_BSS = x_train[m_train_BSS:], y_train[m_train_BSS:]\n",
    "\n",
    "\n",
    "nsub = 4\n",
    "features_idx_dict, validation_err_dict = {}, {}\n",
    "for k in range(1,nsub + 1):\n",
    "    features_idx = list(itertools.combinations(range(x_train_BSS.shape[1]), k))\n",
    "    validation_error = np.zeros(len(features_idx),)\n",
    "    for j in range(len(features_idx)):\n",
    "        # You should use the function you built in previous TODO\n",
    "        # YOUR CODE HERE\n",
    "        COD_train_BSS, COD_val_BSS, w_LS_BSS = solve_LS_problem(x_train_BSS[:, features_idx[j]], y_train_BSS,\n",
    "                                                                  x_val_BSS[:, features_idx[j]], y_val_BSS)\n",
    "        validation_error[j] = 1 - COD_val_BSS\n",
    "    print(f'Number of models trained for {k} chosen features: {len(features_idx)}')\n",
    "\n",
    "    features_idx_dict.update({k: features_idx})\n",
    "    validation_err_dict.update({k: validation_error})\n",
    "\n",
    "validation_err_min_per_size = {k+1: np.min(val_errs) for k, val_errs in enumerate(validation_err_dict.values())}\n",
    "validation_err_argmin_per_size = {k+1: np.argmin(val_errs) for k, val_errs in enumerate(validation_err_dict.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "LfNlI08K2d7u",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d8d0bcff4e04505acc92984cc70787b",
     "grade": true,
     "grade_id": "cell-89e5af2635145e91",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "number_subsets_per_group = [int(scipy.special.comb(17, k)) for k in range(1, nsub + 1)]\n",
    "for k, n_SubSet in zip(range(1, nsub + 1), number_subsets_per_group):\n",
    "    assert len(validation_err_dict[k]) == n_SubSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "deletable": false,
    "editable": false,
    "id": "9juc9Eyf2d7u",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8394f2b573896cd94e25c98b35f2762c",
     "grade": false,
     "grade_id": "cell-a73c8b9fcd2a738e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "5f2cf92d-e81a-4d29-a0b6-92c5a809b937"
   },
   "outputs": [],
   "source": [
    "# Plot the validation score for each model\n",
    "plt.figure(2)\n",
    "for k in range(1,nsub+1):\n",
    "    plt.scatter(k*np.ones(validation_err_dict[k].shape), validation_err_dict[k], color='k', alpha=0.5)\n",
    "    if k > 1:\n",
    "        plt.plot([k-1, k], [validation_err_min_per_size[k-1], validation_err_min_per_size[k]], color='r',marker='o',\n",
    "            markeredgecolor='k', markerfacecolor = 'r', markersize = 10)\n",
    "plt.xlabel('Number of retained features')\n",
    "plt.ylabel('RSS/TSS')\n",
    "plt.title('Best-Subset Selection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "mqbIh_A_2d7u",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f33073179f103753b6cfbb378db36523",
     "grade": false,
     "grade_id": "cell-54e3ececa4f1c681",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "7f50c668-873f-49b2-aa1c-fc8f23552d1c"
   },
   "outputs": [],
   "source": [
    "# TODO 6\n",
    "# Pick the number of features for the best subset according to figure above, select the best subset using the results\n",
    "# above and learn the model on the entire training data (x_train); eventually compute COD on training (x_train) and\n",
    "# on test data (x_test).\n",
    "\n",
    "# Now pick the number of features according to best subset\n",
    "opt_num_features = 4\n",
    "opt_features_idx = features_idx_dict[opt_num_features][validation_err_argmin_per_size[opt_num_features]]\n",
    "\n",
    "# You should use the function you built in previous TODOs\n",
    "COD_train_BSS, COD_test_BSS, w_hat_BSS = None, None, None # Replace with the proper quantities\n",
    "# YOUR CODE HERE\n",
    "COD_train_BSS, COD_test_BSS, w_hat_BSS = solve_LS_problem(x_train[:, opt_features_idx], y_train,\n",
    "                                                                  x_test[:, opt_features_idx], y_test)\n",
    "# Let's print the indices of the features from best subset\n",
    "print(f'Best features indexes: {opt_features_idx}')\n",
    "print(f'Best features names: {str({features_names[i] for i in opt_features_idx})}')\n",
    "\n",
    "# Let's print model performance on train and test datasets (measured using COD)\n",
    "print(f\"Coefficient of determination on training data: {COD_train_BSS:.4f}\")\n",
    "print(f\"Coefficient of determination on test data:     {COD_test_BSS:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "QS-bFb3J2d7v",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82bc763e6ea39796b477fbbbf7ed2e67",
     "grade": true,
     "grade_id": "cell-10a6bfda6755d592",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(opt_features_idx) == opt_num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uZQzvlUe2d7v",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b59e5498891175cfa523e6786356424",
     "grade": false,
     "grade_id": "cell-c964e7ad026bc06a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ridge regression\n",
    "\n",
    "Recall that for linear models with scalar output we have $h(x) = <w,x>$ and that the Empirical error $L_S(h)$ can be written\n",
    "in terms of the vector of parameters $w$, in the form:\n",
    "$$\n",
    "L_S(w) = \\frac{1}{m_t} \\|Y - X w\\|^2\n",
    "$$\n",
    "where $Y$ and $X$ are the matrices whose $i-$th rows are, respectively, the output data $y_i$ and the input vectors $x_i^\\top$.\n",
    "\n",
    "In the case of Ridge regression we add a regularization term to the RSS term so that our Empirical error becomes:\n",
    "$$\n",
    "L_S(w) = \\frac{1}{m_t} \\|Y - X w\\|^2 + \\lambda \\|w\\|^2 \\propto \\|Y - X w\\|^2 + \\underbrace{\\lambda * m_t}_{:=\\alpha} \\|w\\|^2\n",
    "$$\n",
    "The Ridge Least Squares solution is given by the expression:\n",
    "$$\n",
    "\\hat w_{Ridge} = {\\rm arg\\;min}_w L_S(w) = (X^\\top X + \\alpha I)^{-1} X^\\top Y\n",
    "$$\n",
    "\n",
    "__Note__: what has changed w.r.t. the LS solution? Do we need to worry about invertibility of the matrix we need to invert?\n",
    "- Prove that adding a positive multiple of identity to a semi definite positive matrix you get a positive definite matrix.\n",
    "- Prove that a positive definite matrix is *always* invertible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "VNsbrxqN2d7v",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8581f6991b97ea63006062eb771fbbe4",
     "grade": false,
     "grade_id": "cell-5fc49ce00c6175ad",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "dc6a8cb9-b65d-486a-a4b4-5c5fe28d8b07"
   },
   "outputs": [],
   "source": [
    "# TODO 7\n",
    "# Write a function which computes the optimal parameters w_hat, solution to the LS regularized problem described\n",
    "# earlier.\n",
    "# We assume w_hat contains the bias term b (as described in class), so you will need to create the proper input data\n",
    "# adding a fictitious feature containing only ones (it is assumed you add the ones vector before all your features\n",
    "# so that b_hat is in the first position of w_hat).\n",
    "# Then write a function \"solve_ridge_LS_problem\", similar in spirit to \"solve_LS_problem\" (see function description\n",
    "# for details on the input and return values).\n",
    "def compute_LS_optimal_ridge_ERM_coefficients(x_train : np.ndarray, y_train : np.ndarray, alpha : float) -> np.ndarray:\n",
    "    '''\n",
    "    This function estimates the optimal Ridge LS coefficients given the input and output training data x, y.\n",
    "    This function assumes the bias term b is condensed with the other coefficients, therefore a column of ones is\n",
    "    stacked (place it in front of the feature vector) to the input features and the size of the returned optimal\n",
    "    coefficient is: number of features + 1\n",
    "\n",
    "    :param x_train: input features\n",
    "    :param y_train: output to be predicted\n",
    "    :param alpha: regularization parameter\n",
    "\n",
    "    :returns: a column vector containing w_hat, solution to the ridge ERM LS problem\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # add coloumn of ones in the beginning of the feature vector\n",
    "    x_train = add_bias_column(x_train)\n",
    "\n",
    "    # X^TX and X^TY\n",
    "    A = np.matmul(x_train.T, x_train)\n",
    "    B = np.matmul(x_train.T, y_train)\n",
    "\n",
    "    # calculation of the alpha matrix\n",
    "    alpha_matrix = np.zeros((x_train.shape[1], x_train.shape[1]))\n",
    "    np.fill_diagonal(alpha_matrix, alpha)\n",
    "\n",
    "    # w_hat calculation\n",
    "    w_hat = np.matmul( np.linalg.inv(A + alpha_matrix), B)\n",
    "\n",
    "    return w_hat\n",
    "\n",
    "\n",
    "def solve_ridge_LS_problem(x_train : np.ndarray, y_train : np.array, x_test : np.ndarray, y_test : np.array,\n",
    "                           alpha : float) -> tuple:\n",
    "    '''\n",
    "    Funtion used to compute the ridge LS estimate given train data and alpha hyper-parameter.\n",
    "    This function uses Scikit-learn to get both the ridge LS solution and other required quantities.\n",
    "    Note you could have implemented this function using your own \"compute_LS_optimal_ridge_ERM_coefficients\"\n",
    "    but it is faster and easier for you to compute training loss and test one using Scikit-learn implementation.\n",
    "    :param x_train: input data used to get the linear model predictions\n",
    "    :param y_train: output data to be predicted\n",
    "    :param x_test: test features used to assess model performance\n",
    "    :param y_test: test output to be predicted to assess model performance\n",
    "    :param alpha: regularization hyper-parameter\n",
    "\n",
    "    :returns: (COD_train, COD_test, w)\n",
    "        WHERE\n",
    "        COD_train : Coefficient of determination for the training dataset\n",
    "        COD_test : Coefficient of determination for the test dataset\n",
    "        w : parameters of the linear model (the bias is contained, return it as the first element of w)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
    "    ridge_model.fit(x_train, y_train)\n",
    "\n",
    "    # concatenate intercept with coefficients\n",
    "    w = np.insert(ridge_model.coef_, 0, ridge_model.intercept_)\n",
    "\n",
    "    # calculation of CODs\n",
    "    COD_train = ridge_model.score(x_train, y_train)\n",
    "    COD_test = ridge_model.score(x_test, y_test)\n",
    "\n",
    "    return (COD_train, COD_test, w)\n",
    "\n",
    "\n",
    "alpha = 0.1\n",
    "w_hat_ridge_hand = compute_LS_optimal_ridge_ERM_coefficients(x_train, y_train, alpha = alpha)\n",
    "print(f\"w_hat \\n {w_hat_ridge_hand}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "DUtu7wRF2d7w",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17a66279acc43c47f71f77de5aa15739",
     "grade": true,
     "grade_id": "cell-6c4820d46c1d7976",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "94104046-d52e-4204-e18f-d7a61a4fda4b"
   },
   "outputs": [],
   "source": [
    "# Compare your ridge regression solution with sklearn one.\n",
    "# The following lines are also a hint on how to create the function 'solve_ridge_LS_problem'\n",
    "ridge = linear_model.Ridge(alpha=alpha)\n",
    "ridge.fit(x_train, y_train)\n",
    "ridge.coef_.shape, ridge.intercept_.shape\n",
    "w_hat_ridge_sklearn = np.concatenate((ridge.intercept_.reshape(-1,), ridge.coef_))\n",
    "\n",
    "print(f\"w_hat_sklearn \\n {w_hat_ridge_sklearn}\")\n",
    "COD_train_ridge, COD_test_ridge, w_hat_ridge = solve_ridge_LS_problem(x_train, y_train, x_test, y_test, alpha)\n",
    "\n",
    "# Let's print model performance on train and test datasets (measured using COD)\n",
    "print(f\"Coefficient of determination on training data: {COD_train_ridge:.4f}\")\n",
    "print(f\"Coefficient of determination on test data:     {COD_test_ridge:.4f}\")\n",
    "\n",
    "assert np.isclose(w_hat_ridge_hand, w_hat_ridge_sklearn, atol=1e-8).all()\n",
    "assert np.isclose(w_hat_ridge_hand, w_hat_ridge, atol=1e-8).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "deletable": false,
    "editable": false,
    "id": "eQCKtkk_2d7w",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c15ee67edb3cc2d7b7dace8244fefcf",
     "grade": false,
     "grade_id": "cell-467c4be23140b26f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "1d6cbf6a-6018-478c-f0df-c02925518450"
   },
   "outputs": [],
   "source": [
    "# Let's print model train and test metric as a function of the regularization parameter alpha (which is constraining\n",
    "# model complexity, the higher the norm of w the higher model complexity)\n",
    "\n",
    "alphas = np.logspace(-2, 6, 100)\n",
    "ridge_results = [solve_ridge_LS_problem(x_train, y_train, x_test, y_test, a) for a in alphas]\n",
    "\n",
    "train_CODs = list(zip(*ridge_results))[0]\n",
    "test_CODs = list(zip(*ridge_results))[1]\n",
    "all_w_hat = list(zip(*ridge_results))[2]\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(15, 5))\n",
    "axes[0].plot(alphas, list(map(np.linalg.norm, all_w_hat)), label='w norm')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlabel('alpha')\n",
    "axes[0].set_ylabel('Model complexity')\n",
    "\n",
    "axes[1].plot(alphas, train_CODs, label='Train COD')\n",
    "axes[1].plot(alphas, test_CODs, label='Test COD')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel('alpha')\n",
    "axes[1].set_ylabel('COD')\n",
    "\n",
    "# If you would like to see these plots in linear scale in x or y or both just comment the following lines\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_yscale('log')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "4UIq5jZO2d7w",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03b6a2b0478db65315e0fb104396e06c",
     "grade": false,
     "grade_id": "cell-5f9db2b9bcab879b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 8\n",
    "# How to choose the optimal alpha? This is an hyper-parameter, usually it is estimated through Cross-validation.\n",
    "# In this TODO we will implement by hand the Cross Validation procedure to estimate hyper-parameter alpha.\n",
    "# The function we are going to implement is pretty general and can be applied both to Ridge and Lasso (see next)!\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def CV_by_hand(num_folds, model_class, other_model_hyper_parameters, loss, hyper_param_range, x_train, y_train):\n",
    "    kf = KFold(n_splits = num_folds)\n",
    "    loss_kfold = np.zeros(len(hyper_param_range),)\n",
    "    for i in range(len(hyper_param_range)):\n",
    "        for train_index, validation_index in kf.split(x_train):\n",
    "            # In order to complete the following 2 lines have a look at the documentation of\n",
    "            # sklearn.model_selection.KFold (no need to insert new lines, just replace \"None\" with the correct\n",
    "            # quantity)\n",
    "            x_train_kfold, x_val_kfold = x_train[train_index], x_train[validation_index]\n",
    "            y_train_kfold, y_val_kfold = y_train[train_index], y_train[validation_index]\n",
    "            # YOUR CODE HERE\n",
    "            # raise NotImplementedError() # Remove this line\n",
    "\n",
    "            # Initialize the model with the hyper-parameters you are willing to test (in this case we are interested\n",
    "            # on alpha alone, but we might need to change the number of iterations (or other hyper-parameters,\n",
    "            # depending on the model we are using: Ridge, Lasso, etc.) to solve the ERM problem. Therefore we need\n",
    "            # to pass such an information using the dictionary \"other_model_hyper_parameters\")\n",
    "            model_kfold = model_class(alpha=hyper_param_range[i], **other_model_hyper_parameters)\n",
    "\n",
    "            # Fit the model using training data from the k-fold\n",
    "            # YOUR CODE HERE\n",
    "            # raise NotImplementedError() # Remove this line\n",
    "            model_kfold.fit(x_train_kfold, y_train_kfold)\n",
    "\n",
    "            # Compute the loss using the validation data from the k-fold\n",
    "            loss_kfold[i] += loss(y_val_kfold, x_val_kfold, model_kfold)\n",
    "\n",
    "    loss_kfold /= m_t\n",
    "\n",
    "    return loss_kfold\n",
    "\n",
    "# TODO 9: select a good set of 100 alphas to be tested, hint np.logspace(...)\n",
    "\n",
    "n_alphas, num_folds = 100, 5\n",
    "alphas = None # Replace with a proper interval (no need to add any line)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "alphas = np.logspace(0.0, 1.6, num = 100) # 100 points 0 G.G\n",
    "# alphas = np.logspace(1.40, 1.50, num = 100) # 100 points\n",
    "\n",
    "\n",
    "model_class = linear_model.Ridge\n",
    "other_model_hyper_parameters = {}\n",
    "loss = lambda y_val, x_val, model: np.linalg.norm(y_val - model.predict(x_val))**2\n",
    "\n",
    "# Perform CV with your implemented function\n",
    "loss_ridge_kfold = CV_by_hand(num_folds, model_class, other_model_hyper_parameters, loss, alphas, x_train, y_train)\n",
    "\n",
    "assert loss_ridge_kfold.shape == (n_alphas,)\n",
    "\n",
    "# Choose the regularization parameter that minimizes the validation loss\n",
    "best_index, ridge_alpha_opt = None, None # Replace with the correct quatities\n",
    "# YOUR CODE HERE\n",
    "best_index = np.argmin(loss_ridge_kfold)\n",
    "ridge_alpha_opt = alphas[best_index]\n",
    "# print(loss_ridge_kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "_QtWifDL2d7x",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "902e8ad0e666f635288685ad477548cd",
     "grade": true,
     "grade_id": "cell-b7eefb0c254a1c02",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "af1c9f45-7bbf-4b03-afdb-056218c9811a"
   },
   "outputs": [],
   "source": [
    "COD_train_ridge_opt, COD_test_ridge_opt, w_hat_ridge_opt = solve_ridge_LS_problem(x_train, y_train, x_test, y_test, ridge_alpha_opt)\n",
    "\n",
    "print(f\"Best value of the regularization parameter: {ridge_alpha_opt:.4f}\")\n",
    "# Let's print model performance on train and test datasets (measured using COD)\n",
    "print(f\"Coefficient of determination on training data: {COD_train_ridge_opt:.4f}\")\n",
    "print(f\"Coefficient of determination on test data:     {COD_test_ridge_opt:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "deletable": false,
    "editable": false,
    "id": "apik-NHp2d7x",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1f030420533ae834b05d00e651d5984",
     "grade": false,
     "grade_id": "cell-1aba6b630eed1045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "583dbc68-4664-4de5-c7f8-e6824d9e55c2"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(alphas, loss_ridge_kfold, color='b')\n",
    "plt.scatter(ridge_alpha_opt, loss_ridge_kfold[best_index], color='b', marker='o', linewidths=5)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.title('Ridge: choice of regularization parameter alpha')\n",
    "plt.show()\n",
    "\n",
    "COD_train_ridge_opt, COD_test_ridge_opt, w_hat_ridge_opt = solve_ridge_LS_problem(x_train, y_train, x_test, y_test,\n",
    "                                                                                  ridge_alpha_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BRTYz6AS2d7x",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f82677c1069d433bfe612e0a47fee26b",
     "grade": false,
     "grade_id": "cell-ecd13d44183f5345",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## LASSO\n",
    "\n",
    "In the following we will apply a different regularization to our linear model: LASSO - Least Absolute Shrinkage and Selection Operator (l1 regularization)\n",
    "\n",
    "You will code the same function as above, using functions from the Scikit-learn module, to solve the lasso LS problem for a fixed value of the regularization hyper-parameter.\n",
    "\n",
    "After that, use the routine `lasso_path` from `sklearn.linear_regression` to compute the \"lasso path\" for different values of the regularization parameter $\\lambda$. You should first fix a grid of possible values of lambda (the variable `lasso_lams`). For each entry of the vector `lasso_lams` you should compute the corresponding model (The $i-$th column of the vector  `lasso_coefs` should contain the coefficients of the linear model computed using `lasso_lams[i]` as regularization parameter).\n",
    "\n",
    "Be careful that the grid should be chosen appropriately.\n",
    "\n",
    "**Note**: the parameter $\\lambda$ is called $\\alpha$ in the Lasso model from sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "2AgvRJ2Q2d7x",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b156dca3d03fc5075c8f519e2710fa12",
     "grade": false,
     "grade_id": "cell-9bfb05d8b6349507",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "b9e7713f-e58d-4c41-9125-aaadbc9b7ec6"
   },
   "outputs": [],
   "source": [
    "# TODO 9\n",
    "# As we did for ridge regression and LS, write a function to solve the Lasso LS Problem exploiting sklearn\n",
    "def solve_lasso_LS_problem(x_train : np.ndarray, y_train : np.array, x_test : np.ndarray, y_test : np.array,\n",
    "                           lam : float) -> tuple:\n",
    "    '''\n",
    "    Funtion used to compute the LASSO LS estimate given train data and lambda hyper-parameter.\n",
    "    This function uses Scikit-learn to get both the LASSO LS solution and other required quantities.\n",
    "    :param x_train: input data used to get the linear model predictions\n",
    "    :param y_train: output data to be predicted\n",
    "    :param x_test: test features used to assess model performance\n",
    "    :param y_test: test output to be predicted to assess model performance\n",
    "    :param lam: regularization hyper-parameter (what is called alphas in sklearn)\n",
    "\n",
    "    :returns: (COD_train, COD_test, w)\n",
    "        WHERE\n",
    "        COD_train : Coefficient of determination for the training dataset\n",
    "        COD_test : Coefficient of determination for the test dataset\n",
    "        w : parameters of the linear model (the bias is contained, return it as the first element of w)\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    lasso_model = linear_model.Lasso(alpha=lam)\n",
    "    lasso_model.fit(x_train, y_train)\n",
    "\n",
    "    # concatenate intercept with coefficients\n",
    "    w = np.insert(lasso_model.coef_, 0, lasso_model.intercept_)\n",
    "\n",
    "    # calculation of CODs\n",
    "    COD_train = lasso_model.score(x_train, y_train)\n",
    "    COD_test = lasso_model.score(x_test, y_test)\n",
    "\n",
    "    return (COD_train, COD_test, w)\n",
    "\n",
    "\n",
    "lam = 0.1\n",
    "COD_train_lasso, COD_test_lasso, w_hat_lasso = solve_lasso_LS_problem(x_train, y_train, x_test, y_test, lam)\n",
    "\n",
    "# Let's print model performance on train and test datasets (measured using COD)\n",
    "print(f\"Coefficient of determination on training data: {COD_train_lasso:.4f}\")\n",
    "print(f\"Coefficient of determination on test data:     {COD_test_lasso:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CKNmEflv2d7x",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85d716e21adb55f790e03fc13070a203",
     "grade": true,
     "grade_id": "cell-60bf190321d932f2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert w_hat_lasso.shape == (18,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "UzeaMFA32d7x",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16e27157462960d9c00278cc82f6c4df",
     "grade": false,
     "grade_id": "cell-ff1fbf7081f27b3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 10\n",
    "from sklearn.linear_model import lasso_path\n",
    "# Select a grid of possible regularization parameters (be careful how this is chosen, you may have to refine\n",
    "# the choice after seeing the results)\n",
    "\n",
    "num_lambdas = 100\n",
    "lasso_lams = None # Replace with a proper interval (no need to add any line)\n",
    "# YOUR CODE HERE\n",
    "# as we use -np.log10\n",
    "# lasso_lams = np.logspace(0.5, -2.5, 100)\n",
    "lasso_lams = np.logspace(0, -3, 100)\n",
    "\n",
    "# Use the function lasso_path (see documentation) to compute the \"lasso path\", passing as input the lambda values\n",
    "# you have specified in lasso_lams\n",
    "# YOUR CODE HERE\n",
    "_, lasso_coefs, _ = lasso_path(x_train, y_train, alphas=lasso_lams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JdoejtTD2d7x",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41d1219849b0db45a3daabe7ff13aadd",
     "grade": true,
     "grade_id": "cell-ab25d44772351a11",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert lasso_coefs.shape == (17, num_lambdas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PeKW0Uae2d7x",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1c020d175940ec8335fdcb172b967bf",
     "grade": false,
     "grade_id": "cell-bbb0c5fcb2c3638d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Evaluate the sparsity in the estimated coefficients as a function of the regularization parameter $\\lambda$: to this purpose, compute the number of non-zero entries in the estimated coefficient vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "deletable": false,
    "id": "3G5ubQ9h2d7y",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e91a2e94fbc9dff3850549c4a62cfbb0",
     "grade": false,
     "grade_id": "cell-633bd325ee7dcb00",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "outputId": "2556f20f-8ac0-4dd8-8238-3c3214603c33"
   },
   "outputs": [],
   "source": [
    "number_non_zero_coeffs = np.zeros(len(lasso_lams),)\n",
    "# The number of non zero coeffs must be evaluated for each lambda\n",
    "# YOUR CODE HERE\n",
    "# list comprehension that checks each coef in each col and stores the nr of non-zero elements in the col\n",
    "number_non_zero_coeffs = [np.count_nonzero(lasso_coefs[:, lambda_col]) for lambda_col in range(lasso_coefs.shape[1])]\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(15, 5))\n",
    "axes[0].plot(lasso_lams, number_non_zero_coeffs, marker='o', markersize=5)\n",
    "axes[0].set_xlabel('Lambda')\n",
    "axes[0].set_ylabel('Number of non-zero coefficients')\n",
    "axes[0].set_title('Sparsity Degree')\n",
    "\n",
    "neg_log_alphas_lasso = -np.log10(lasso_lams) # This is used only to make a nice plot (you can directly use: lasso_lams as x value)\n",
    "for i, coef_l in enumerate(lasso_coefs):\n",
    "    l1 = axes[1].plot(neg_log_alphas_lasso, coef_l, label=features_names[i])\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1))\n",
    "axes[1].set_xlabel('-log_10(lambda)')\n",
    "axes[1].set_ylabel('i-th parameter value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wQq5urlh2d7y",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1bc3aacf134448fa0e44592c9e034be",
     "grade": true,
     "grade_id": "cell-a20d10d6827fc124",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(number_non_zero_coeffs) == num_lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "XZ0DoPPP2d7y",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77f0eeb946cff3fc8eb1b6f94a5132c2",
     "grade": false,
     "grade_id": "cell-c5665d39a0835c88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TODO 11: explain the results in the figures above (max 5 lines)\n",
    "What does each plot mean? Did you observe what you would have expected from the theory?\n",
    "Type your answer in the next cell (no code needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "IGtr7jD62d7z",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "604311d16828d3532160294af528c749",
     "grade": true,
     "grade_id": "cell-608b39d4f06e11b4",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "- **Left PLot**\n",
    "This plot helps us in visualizing how when the $λ$ hyperparameter increases, the penalty is more severe and so we have more zero coefficients. LASSO operates in such a way that it sets certain coefficients to 0 depending on how high the penalty value (propotional to regularization) is at that current moment. Geometrically speaking this is the point where the corners of the constraint region (hypercube) defined by the $l^1$ norm actually intersects with the regression hyperplane. \n",
    "\n",
    "- **Right Plot**\n",
    "This plot shows us how every feature reacts to a decrease in the penalty term (as its a $-\\log_{10}$ plot) in the beginning of the plot the value of $λ$ is at its highest and the regularization effect is at its peak, zeroing out all coefficients, and as the value of $\\lambda$ decreases (regularization decreases) we see that slowly each of the ceofficients start being different from 0 and gaining importance in the prediction.\n",
    "\n",
    "- **Did we observe what you have expected from theory?**\n",
    "Yes we did see what we had observed, as explained above, but what we didn't see in theory is that the first feature (in this case its **\"sqft_living\"**) that starts to be different from 0 and reaches a peak and gains importance also converges to back to 0 as it looses importance when regularization decreases and other features gain more importance over it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "BvhRhCGu2d7z",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6314a073621d2b30b176c5aba65042e4",
     "grade": false,
     "grade_id": "cell-f50a8eb060113fb2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 12\n",
    "# Use Cross-Validation to find the optimal lam. You should use the CV function you implemented earlier.\n",
    "# Once the omptimal lambda has been found, the following cell will automatically print its training and test error\n",
    "num_folds, num_lambdas = 5, 100\n",
    "lambdas = None # Replace with a proper interval (no need to add any line)\n",
    "# YOUR CODE HERE\n",
    "# lambdas = np.logspace(1, -2.5, 100) GG almost 0\n",
    "# lambdas = np.logspace(-1.1, -1.5, 100)\n",
    "lambdas = np.logspace(0, -3, 100)\n",
    "\n",
    "\n",
    "model_class = linear_model.Lasso\n",
    "other_model_hyper_parameters = {} # {'max_iter':10000} use this if you want to increase the number of iteration\n",
    "                                  # of the optimization: not required\n",
    "loss = lambda y_val, x_val, model: np.linalg.norm(y_val - model.predict(x_val))**2\n",
    "\n",
    "# Perform CV with your implemented function\n",
    "loss_lasso_kfold = CV_by_hand(num_folds, model_class, other_model_hyper_parameters, loss, lambdas, x_train, y_train)\n",
    "\n",
    "# Do not worry if you get the warning (\"Objective did not converge\"). You may try to increase the number of iterations,\n",
    "# but the execution time will increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "deletable": false,
    "editable": false,
    "id": "OEV0eCmj2d7z",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fe1210597574280a078d5b708d9bcc5",
     "grade": true,
     "grade_id": "cell-f9bdd9d0d3e78fa6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "2d39ad13-623e-4175-a7ba-f31e7e15a9fb"
   },
   "outputs": [],
   "source": [
    "assert loss_lasso_kfold.shape == (num_lambdas,)\n",
    "\n",
    "# Choose the regularization parameter that minimizes the validation loss\n",
    "best_index = np.argmin(loss_lasso_kfold)\n",
    "lasso_lambda_opt = lambdas[best_index]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambdas, loss_lasso_kfold, color='b')\n",
    "plt.scatter(lasso_lambda_opt, loss_lasso_kfold[best_index], color='b', marker='o', linewidths=5)\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('Validation Error')\n",
    "plt.title('LASSO: choice of regularization parameter lambda')\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "COD_train_lasso_opt, COD_test_lasso_opt, w_hat_lasso_opt = solve_lasso_LS_problem(x_train, y_train, x_test, y_test,\n",
    "                                                                                  lasso_lambda_opt)\n",
    "\n",
    "print(f\"Best value of the regularization parameter: {lasso_lambda_opt:.4f}\")\n",
    "# Let's print model performance on train and test datasets (measured using COD)\n",
    "print(f\"Coefficient of determination on training data: {COD_train_lasso_opt:.4f}\")\n",
    "print(f\"Coefficient of determination on test data:     {COD_test_lasso_opt:.4f}\")\n",
    "print(\"Total number of coefficients:\", len(w_hat_lasso_opt))\n",
    "print(\"Number of non-zero coefficients:\", sum(w_hat_lasso_opt != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "deletable": false,
    "editable": false,
    "id": "8a8ImxCj2d7z",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dad210fa517f7164a9d492b072c52aa",
     "grade": false,
     "grade_id": "cell-8c306a88edde0aca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c3b06ed4-e4a3-42ee-f289-13e84ffa6383"
   },
   "outputs": [],
   "source": [
    "# Let's print some performance metrics of the models we defined throughout the notebook\n",
    "columns = ['COD_Train', 'COD_Test']\n",
    "dict_results = {'LS_full': [COD_train_LS_full, COD_test_LS_full],\n",
    "                'LS_reduced_hand': [COD_train_LS_reduced, COD_test_LS_reduced],\n",
    "                'LS_reduced_BSS': [COD_train_BSS, COD_test_BSS],\n",
    "                'ridge_opt': [COD_train_ridge_opt, COD_test_ridge_opt],\n",
    "                'lasso_opt': [COD_train_lasso_opt, COD_test_lasso_opt]\n",
    "               }\n",
    "results = pd.DataFrame.from_dict(dict_results, orient='index', columns=columns)\n",
    "results['Gen_gap'] = np.abs(results['COD_Train'] - results['COD_Test'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "deletable": false,
    "editable": false,
    "id": "TaM9m-Yz2d7z",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae7288a29a63afdfe04c6e63c72446f4",
     "grade": false,
     "grade_id": "cell-ebce3e4ffa25e850",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "339d39b8-dbc3-4b44-be56-f4cf8a91bee2"
   },
   "outputs": [],
   "source": [
    "# Let's compare the final coefficients\n",
    "ind = np.arange(1, len(w_hat_lasso_opt) + 1)  # the x locations for the groups\n",
    "width = 0.25       # the width of the bars\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "rects1 = ax.bar(ind,             w_LS_full,        width, color='r',\n",
    "                label=f'LS norm_coeff {np.linalg.norm(w_LS_full):.4}, # non-zero coeffs {sum(w_LS_full != 0)}')\n",
    "rects2 = ax.bar(ind + width,     w_hat_ridge_opt, width, color='y',\n",
    "               label=f'Ridge norm_coeff {np.linalg.norm(w_hat_ridge_opt):.4} # non-zero coeffs {sum(w_hat_ridge_opt != 0)}')\n",
    "rects3 = ax.bar(ind + 2 * width, w_hat_lasso_opt, width, color='g',\n",
    "               label=f'Lasso norm_coeff {np.linalg.norm(w_hat_lasso_opt):.4} # non-zero coeffs {sum(w_hat_lasso_opt != 0)}')\n",
    "plt.xlabel('Coefficient Idx')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('LS, Ridge and Lasso Coefficient')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4P13Agva2d70",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "389efe866f4617b312707ea03e722538",
     "grade": false,
     "grade_id": "cell-94d9980fe36febe4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TODO 13\n",
    "\n",
    "Compare and comment the results obtained so far: did you get what you would have expected from the theory?\n",
    "Type your answer in the next cell (no code needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "hCe0ud0z2d70",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e8ae9a358b9ae1a16b968502595be2f",
     "grade": true,
     "grade_id": "cell-65b6eb5e45308bd4",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Yes, the results do coincide well with theory. \n",
    "1. We can observe that ordinary **LS_full** and **LS_reduced_BSS** work well, but are not able to get the generalization gap down to what is possible with **lasso_opt** and **ridge_opt**, both these regularization techniques are able to get the generalization gap down by 3x in comparison to **LS_full** and preventing overfitting on the training data, without compromising heavily on the performance of the model. \n",
    "\n",
    "2. When we compare the final coefficients provided by each model via the histogram, what one can immediately notice is how the **lasso** regularization technique has the ability to zero out certain coefficients and effectively selecting the non-zero ones, for example in our case, it zeros out 7 coefficients because of the optimal choice of the regularization parameter $\\lambda$ that was found from cross-validation and how the penalty function defined by the  $||w||_1$ Manhattan norm shapes its contraint boundries whereas with **ridge** that is not the case as, the penalty function is the Eucledian norm $||w||_2^2$ which is defined as a sphere in the feature space in comparison to the hypercube defined by the $||w||_1$ norm defined in **lasso**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Lwh0v7M82d70",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0a7fabfdb70342235dfd59ec15735ec",
     "grade": false,
     "grade_id": "cell-a26f4e70b97f612a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### How do LS, Ridge and LASSO reject redundant/useless features?\n",
    "In the next TODOs we are going to create two new hand-crafted datasets:\n",
    "- first we simply replicate the same features a certain number of times (so that we will have redundant features)\n",
    "- second we simply replicate same features and we add some noise over them\n",
    "\n",
    "Let's see how the models we studied so far behaves in presence of this kind of nuisances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5By1ywnc2d70",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b9c9aaf4e4c539315f14a159d4b132d",
     "grade": false,
     "grade_id": "cell-f8d6035161675a88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's first create a function to train and print in one shot all the quantities we are interested on: Training COD,\n",
    "# Test COD, model parameters.\n",
    "\n",
    "def solve_Ls_Ridge_Lasso(x_train : np.ndarray, y_train : np.array, x_test : np.ndarray, y_test : np.array):\n",
    "    # Solve Ordinary LS\n",
    "    COD_train_LS_full, COD_test_LS_full, w_LS_full = solve_LS_problem(x_train, y_train, x_test, y_test)\n",
    "    num_folds, n_alphas, num_lambdas = 5, 100, 100\n",
    "    loss = lambda y_val, x_val, model: np.linalg.norm(y_val - model.predict(x_val))**2\n",
    "\n",
    "    # Solve Ridge\n",
    "    alphas = np.logspace(-3,2.5, num = n_alphas)\n",
    "    loss_ridge_kfold = CV_by_hand(num_folds, linear_model.Ridge, {}, loss, alphas, x_train, y_train)\n",
    "    best_index_ridge = np.argmin(loss_ridge_kfold)\n",
    "    ridge_alpha_opt = alphas[best_index_ridge]\n",
    "    COD_train_ridge_opt, COD_test_ridge_opt, w_hat_ridge_opt = solve_ridge_LS_problem(x_train, y_train, x_test, y_test,\n",
    "                                                                                      ridge_alpha_opt)\n",
    "\n",
    "    # Solve LASSO\n",
    "    lambdas = np.logspace(-4,0, num = num_lambdas)\n",
    "    loss_lasso_kfold = CV_by_hand(num_folds, linear_model.Lasso, {}, loss, lambdas, x_train, y_train)\n",
    "    best_index = np.argmin(loss_lasso_kfold)\n",
    "    lasso_lambda_opt = lambdas[best_index]\n",
    "    COD_train_lasso_opt, COD_test_lasso_opt, w_hat_lasso_opt = solve_lasso_LS_problem(x_train, y_train, x_test, y_test,\n",
    "                                                                                      lasso_lambda_opt)\n",
    "\n",
    "    # The following is simply a copy and paste of what we have done earlier\n",
    "    columns = ['COD_Train', 'COD_Test','w_opt']\n",
    "    dict_results = {\n",
    "                    'LS_full': [COD_train_LS_full, COD_test_LS_full,w_LS_full],\n",
    "                    'ridge_opt': [COD_train_ridge_opt, COD_test_ridge_opt,w_hat_ridge_opt],\n",
    "                    'lasso_opt': [COD_train_lasso_opt, COD_test_lasso_opt,w_hat_lasso_opt]\n",
    "                   }\n",
    "    results = pd.DataFrame.from_dict(dict_results, orient='index', columns=columns)\n",
    "    results['Gen_gap'] = np.abs(results['COD_Train'] - results['COD_Test'])\n",
    "\n",
    "    # Let's compare the final coefficients\n",
    "    ind = np.arange(1, len(w_hat_lasso_opt) + 1)  # the x locations for the groups\n",
    "    width = 0.25       # the width of the bars\n",
    "    fig, ax = plt.subplots(figsize=(15,7))\n",
    "    rects1 = ax.bar(ind,             w_LS_full,        width, color='r',\n",
    "                    label=f'LS norm_coeff {np.linalg.norm(w_LS_full):.4}, # non-zero coeffs {sum(w_LS_full != 0)}')\n",
    "    rects2 = ax.bar(ind + width,     w_hat_ridge_opt , width, color='y',\n",
    "                   label=f'Ridge norm_coeff {np.linalg.norm(w_hat_ridge_opt):.4} # non-zero coeffs {sum(w_hat_ridge_opt != 0)}')\n",
    "    rects3 = ax.bar(ind + 2 * width, w_hat_lasso_opt , width, color='g',\n",
    "                   label=f'Lasso norm_coeff {np.linalg.norm(w_hat_lasso_opt):.4} # non-zero coeffs {sum(w_hat_lasso_opt != 0)}')\n",
    "    plt.xlabel('Coefficient Idx')\n",
    "    plt.ylabel('Coefficient Value')\n",
    "    plt.title('LS, Ridge and Lasso Coefficient')\n",
    "    plt.legend()\n",
    "\n",
    "    return results, fig, ridge_alpha_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "deletable": false,
    "editable": false,
    "id": "WYI2JJgP2d70",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c303ee265abc61a40024d967201e1af",
     "grade": false,
     "grade_id": "cell-db395087dba2914e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "c4c3fb07-8797-4b29-a68e-06405cb60384"
   },
   "outputs": [],
   "source": [
    "old_results, old_fig, _ = solve_Ls_Ridge_Lasso(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "deletable": false,
    "editable": false,
    "id": "FpkxAttG2d71",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0e1331845ec61108d754b2b5ae89fed",
     "grade": false,
     "grade_id": "cell-156fd97c5233d9f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "e92ed0a0-cd20-42e3-8166-54f0a9000a21"
   },
   "outputs": [],
   "source": [
    "# We should now see the very same results we obtained earlier\n",
    "old_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "cWeV2FS52d71",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f7a2a05a02f0cc0e30bab890effe659",
     "grade": false,
     "grade_id": "cell-c712d1cf8126786e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 14\n",
    "# Write a function that replicates a selected feature from a dataset x and places at the end of the new_x\n",
    "def replicate_a_random_feature(x_train : np.array, x_test : np.array, chosen_feature: int) -> np.array:\n",
    "    '''\n",
    "    This function replicates a chosen feature from the ones of a given dataset and return a dataset\n",
    "    containing all the old features + the copied one (In the last position!) - this operation must be\n",
    "    done for the test dataset too.\n",
    "    :param x_train: Features we are willing to replicate of shape (m_t, n_feats)\n",
    "    :param x_test: Features we are willing to replicate of shape (m_test, n_feats)\n",
    "    :chosen_feature: Index of the feature to be replicated\n",
    "\n",
    "    :returns: (new_x_train, new_x_test)\n",
    "        WHERE\n",
    "            new_x_train: New set of train features with a replicated feature\n",
    "                         (its shape is (m_t, n_feats + 1))\n",
    "            new_x_test: New set of test features with a replicated feature\n",
    "                         (its shape is (m_test, n_feats + 1))\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    new_x_train = np.column_stack((x_train, x_train[: , chosen_feature]))\n",
    "    new_x_test = np.column_stack((x_test, x_test[: , chosen_feature]))\n",
    "\n",
    "    return (new_x_train, new_x_test)\n",
    "\n",
    "# Write a function that replicates a selected feature and adds a gaussian noise over it (with zero mean and variance 0.5)\n",
    "def add_random_feature(x_train : np.array, x_test : np.array, chosen_feature: int) -> np.array:\n",
    "    '''\n",
    "    This function replicates a chosen feature from a given dataset, adds a gaussian noise to it (with zero mean and variance 0.5)\n",
    "    and places such copy at the end of the dataset (this operation must be done for the test dataset too).\n",
    "    The noise applied on the train and test replicated features are extracted from the same gaussian distribution\n",
    "    but they are not the same realization.\n",
    "    :param x_train: Features from the train dataset (m_t, n_feats)\n",
    "    :param x_test: Features from the test dataset (m_test, n_feats)\n",
    "    :chosen_feature: Index of the feature to be replicated\n",
    "\n",
    "    :returns: (new_x_train, new_x_test)\n",
    "        WHERE\n",
    "            new_x_train: New set of train features\n",
    "                         (its shape is (m_t, n_feats + 1))\n",
    "            new_x_test: New set of test features\n",
    "                         (its shape is (m_test, n_feats + 1))\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    variance = 0.5\n",
    "\n",
    "    chosen_feature_train = x_train[: , chosen_feature] +  np.random.normal(loc=0, scale=np.sqrt(variance), size=x_train.shape[0])\n",
    "    chosen_feature_test = x_test[: , chosen_feature] + np.random.normal(loc=0, scale=np.sqrt(variance), size=x_test.shape[0])\n",
    "\n",
    "\n",
    "    new_x_train = np.column_stack((x_train, chosen_feature_train))\n",
    "    new_x_test = np.column_stack((x_test, chosen_feature_test))\n",
    "\n",
    "    return (new_x_train, new_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Rjvl8TPF2d71",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23c68062dfa6eb065420fcca2e40a27e",
     "grade": true,
     "grade_id": "cell-de240ae630a9711a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "replicated_x_train, replicated_x_test = replicate_a_random_feature(x_train, x_test,8)\n",
    "added_x_train, added_x_test = add_random_feature(x_train, x_test,8)\n",
    "\n",
    "assert (x_train.shape[0], x_train.shape[1] + 1) ==  replicated_x_train.shape\n",
    "assert (x_test.shape[0], x_test.shape[1] + 1) ==  replicated_x_test.shape\n",
    "\n",
    "assert (x_train.shape[0], x_train.shape[1] + 1) ==  added_x_train.shape\n",
    "assert (x_test.shape[0], x_test.shape[1] + 1) ==  added_x_test.shape\n",
    "\n",
    "assert np.isclose((added_x_train[:,-1]-x_train[:,8]).mean(), 0., atol=4e-1)\n",
    "assert np.isclose((added_x_train[:,-1]-x_train[:,8]).var(), 0.5, atol=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": false,
    "editable": false,
    "id": "AOXNxcxF2d71",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecd532fffea9a40ddee71b5759c484f4",
     "grade": false,
     "grade_id": "cell-fa29f66c5511bff4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "3ac9ed1d-eb2d-4907-fd60-9d531240170e"
   },
   "outputs": [],
   "source": [
    "replicated_x_train, replicated_x_test = replicate_a_random_feature(x_train, x_test,8)\n",
    "num_duplicated_features, checkpoints = 6, [2,5]\n",
    "results_at_checkpoints, figs_at_checkpoints = [], []\n",
    "for i in range(1, num_duplicated_features):\n",
    "    if i in checkpoints:\n",
    "        results, fig, _ = solve_Ls_Ridge_Lasso(replicated_x_train, y_train, replicated_x_test, y_test)\n",
    "        results_at_checkpoints.append(results)\n",
    "        figs_at_checkpoints.append(fig)\n",
    "    replicated_x_train, replicated_x_test = replicate_a_random_feature(replicated_x_train, replicated_x_test,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "lFHlTJJV2d72",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22312e9a0ff148989b97452510d23bc6",
     "grade": false,
     "grade_id": "cell-9abe530ec86e9043",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "46ef36f5-a8f2-4ebb-c765-ddf383ba9d90"
   },
   "outputs": [],
   "source": [
    "for checkpoint, results in zip(checkpoints, results_at_checkpoints):\n",
    "    print(checkpoint, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "deletable": false,
    "editable": false,
    "id": "gKN7ZFN62d72",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b97b4ee35680a4bda9e8ebdb66f8e51f",
     "grade": false,
     "grade_id": "cell-6c6cd8d737e7f2eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "1694d2cc-5e26-445b-b1a6-344ffeafa8e3"
   },
   "outputs": [],
   "source": [
    "added_x_train, added_x_test = add_random_feature(x_train, x_test,8)\n",
    "num_added_features, checkpoints = 6, [2,5]\n",
    "added_results_at_checkpoints, added_figs_at_checkpoints = [], []\n",
    "for i in range(1, num_added_features):\n",
    "    if i in checkpoints:\n",
    "        results, fig, bbb = solve_Ls_Ridge_Lasso(added_x_train, y_train, added_x_test, y_test)\n",
    "        added_results_at_checkpoints.append(results)\n",
    "        added_figs_at_checkpoints.append(fig)\n",
    "    added_x_train, added_x_test = add_random_feature(added_x_train, added_x_test,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "nnxdbFvQ2d72",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2e813bc8b42f0446998a664770221a2",
     "grade": false,
     "grade_id": "cell-ef9fc256ee8fbbd8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "1db65f37-53b6-4613-ea7a-200a61ea6315"
   },
   "outputs": [],
   "source": [
    "for checkpoint, results in zip(checkpoints, added_results_at_checkpoints):\n",
    "    print(checkpoint, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JCfWq5Pz2d73",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22f1f6a954fba5314894764c281501e7",
     "grade": false,
     "grade_id": "cell-1edff4e8447e6b66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TODO 15\n",
    "\n",
    "Comment in the next cell the results obtained using LS, Ridge and LASSO after we added redundant/useless features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "xSS_hV3V2d73",
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cf32f162d7d7f20a0a2828f08211536",
     "grade": true,
     "grade_id": "cell-e98165df68c2fc47",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "When we add redundant/useless features **without added noise** the results don't vary much for each individual model as more redundant features are added, and the generalization gap stays almost constant when 2 redundant features are added in comparison to 5. The more interesting case here is when **features with noise** are added and we can observe a more clear hindrance on performance. The following in explained below: \n",
    "\n",
    "1. LS: When noisy features are added we can see that the model easily overfits on the training data, as it tries to fit the noise in addition to the signal, and that results in a monotonic increase in the generelization gap (almost doubling from 2 noisy features to 5). LS minimizes the RSS (residual sum of squares) without impossing any penalty on the coefficients (lack of regularization), leading to a model which is **highly sensitive to training data**. \n",
    "\n",
    "2. Ridge: As more noisy features are added the results of the coefficients are not effected so heavily, the generelization gap also increases but not as in the case of LS. This is due to the penalty function defined by the $||w||_2^2$ norm and the correct choice of the regularization parameter. The penalty term reduces the model's sensitivity to individual noisy data points by shriking the coefficients, making the model less prone to overfitting in the presence of noise. \n",
    "\n",
    "3. LASSO: This case has performance somewhere in the middle of LS and Ridge, performing better than ordinary LS but not as well as Ridge. Lasso regression has the $||w||_1$ norm as its penalty and this promotes sparsity in the model. Lasso has the tendency to set some of the variables to zero that can carry meaningful signals. \n",
    "\n",
    "Why does **Ridge perform better than LASSO** in our case? This is due to the fact that the coefficients that we're dealing with have a high tendency of **multicollinearity** and the way in which the $||w||_2^2$ norm adds a penalty is by shriking all the coefficients and taking a sort of average rather than the LASSO approach which is more aggressive and sets certain features to zero, which might miss weak features amidst noise but the ridge approach never sets the coefficients to 0, but reduces the overall bias by shrinking them and relying too heavily on any single feature. **LASSO** provides better regularization when the underlying relationship of the data is sparse and not highly multicollinear. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
