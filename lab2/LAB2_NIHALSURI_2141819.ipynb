{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\" and remove every line containing the expression: \"raise ...\" (if you leave such a line your code will not run).\n",
    "\n",
    "Do not remove any cell from the notebook you downloaded. You can add any number of cells (and remove them if not more necessary).\n",
    "\n",
    "Do not leave any variable initialized to None.\n",
    "\n",
    "## IMPORTANT: make sure to rerun all the code from the beginning to obtain the results for the final version of your notebook, since this is the way we will do it before evaluating your notebook!!!\n",
    "\n",
    "## Make sure to name your notebook file (.ipynb) correctly:\n",
    "### - LAB2_NAMESURNAME_ID (E.g. : LAB2_MARIOROSSI_2204567)\n",
    "\n",
    "## Fill in your name, surname and id number (numero matricola) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22388\\1217988265.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mNAME\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mID_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Your version of IPython is too old, please update it.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "NAME = \"\"\n",
    "ID_number = int(\"\")\n",
    "\n",
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a8a7138092d5b71e0b531b7897df9f68",
     "grade": false,
     "grade_id": "cell-86b3eca1656a3646",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Main topics:\n",
    "- Gaussian random variables and vectors\n",
    "- True vs empirical risk\n",
    "- Least squares regression \n",
    "\n",
    "In this lab you will see how to generate both gaussian random variables and gaussian random vectors.\n",
    "Moreover you will impement the least squares solution for a simple 1-dimensional regression problem.\n",
    "We are going to compare true risk $\\mathcal{L}_D(h)$ and empirical risk $\\mathcal{L}_S(h)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97a92e010d2a6dfe3ab5dd68d0ba0be6",
     "grade": false,
     "grade_id": "cell-a06f2d96ea1cd34f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "163ede060c3233bdd991776d022f205a",
     "grade": false,
     "grade_id": "cell-b46d5a20d1b27cd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Let's generate gaussian random vectors using numpy\n",
    "Generating gaussian random variables or vectors is extremely simple. To begin with, remember that a gaussian distribution is parametrized by its mean and variance (covariance for multidimensional distribution). These two parameters uniquely characterize a gaussian distribution (once you fix them you will obtain the same distribution).\n",
    "\n",
    "For the sake of simplicity let's start with a 1-d random variable. Let's say that we are willing to generate the outcomes $x_i \\in \\mathbb{R}$, $i=1,...,m$, of the gaussian random variable $X\\sim\\mathcal{N}(\\mu, \\sigma^2)$, assuming we know both $\\mu$ and $\\sigma$. \n",
    "\n",
    "There are plenty of ways to generate such outcomes! The easiest way is to exploit a property of gaussian distribution (which we have seen during the probability review): gaussian distributions are closed under affine transformations! This means that if you apply an affine transformation (say $Y:=aX+b$, $a\\in \\mathbb{R}, b \\in \\mathbb{R}$) to a gaussian random variable, the new transformed random variable $Y$ will once again be a gaussian distribution. We transformed the random variable with the affine transformation, therefore the parameters which uniquely describe it (mean and variance) are now different. Is it possible to find the new parameters given the starting parameters and the affine transformation?\n",
    "You should remember these relations by heart, but for the sake of clarity let's recall them.\n",
    "\n",
    "Let $Y:=aX+b$ be the new gaussian random variable.\n",
    "\n",
    "$$\\mathbb{E}[Y] = a \\mathbb{E}[X] + b,$$\n",
    "\n",
    "$$Var[X] = \\mathbb{E}[(aX+b - \\mathbb{E}[aX+b])^2] = a^2\\mathbb{E}[(X - \\mathbb{E}[X])^2] = a^2 Var[X].$$\n",
    "\n",
    "So we have $Y\\sim \\mathcal{N}(a\\mu + b, a^2\\sigma^2)$.\n",
    "\n",
    "Let's consider a tough scenario: say someone asked you to build a gaussian distribution with mean $\\mu_{new}$ and variance $\\sigma^2_{new}$ starting from another gaussian (say $X\\sim\\mathcal{N}(\\mu, \\sigma^2)$), what would you do? \n",
    "The straightforward way would be to apply an affine transformation to $X$, but how to choose the proper affine transformation? \n",
    "You only need to find $a,b$ imposing the following: $\\mu_{new} = a \\mu + b$ and $\\sigma^2_{new} = a^2\\sigma^2$\n",
    "\n",
    "$$\\implies \\ \\begin{cases}a=\\frac{\\sigma_{new}}{\\sigma} \\\\ b = \\mu_{new} - \\frac{\\sigma_{new}}{\\sigma} \\mu \\end{cases}$$\n",
    "\n",
    "In practice it is super easy to generate by means of a computer standard gaussian random variables (zero mean and unit variance). This means that you can create __any__ gaussian random variable starting from a standard one and applying an affine transformation with $a$ and $b$ choosen as:\n",
    "\n",
    "$$a=\\sigma_{new} \\quad \\text{and} \\quad b = \\mu_{new}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c027dcd1e6fc9525a2d35892b93ceb5e",
     "grade": false,
     "grade_id": "cell-513f48a9389c1b87",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 1\n",
    "# Write a function to create a gaussian random variable realization from a gaussian distribution of given mean (mu) \n",
    "# and variance (sigma_2).  \n",
    "# You will need to start from some realizations of a standard gaussian random variable obtained using numpy.\n",
    "def generate_gaussian(mu : float, sigma_2 : float, m : int):\n",
    "    '''\n",
    "    Function which generates m realizations from a gaussian random variable given the desired mean and variance\n",
    "    :param mu: Desired mean\n",
    "    :param sigma_2: Desired variance\n",
    "    :param m: Number of outcomes\n",
    "    :return realizations: Realizations from a gaussian random variable with mean mu and variance sigma_2 \n",
    "                          (numpy array of shape (1, m))\n",
    "    '''\n",
    "    standard_realizations = np.random.normal(size=(1, m))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() # Remove this line\n",
    "    return realizations\n",
    "\n",
    "realizations = generate_gaussian(1, 3, 10000)\n",
    "print(f\"From the realizations we get a sample mean={realizations.mean():.2f} \" \n",
    "                             f\"and sample variance={realizations.var():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d4f5338460fd080c4b3ec18acf6ff93",
     "grade": true,
     "grade_id": "cell-4339b1853040f15a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mu, sigma_2 = 3, 10\n",
    "realizations = generate_gaussian(mu, sigma_2, 1000000)\n",
    "assert np.isclose(realizations.mean(), mu, atol=0.1)\n",
    "assert np.isclose(realizations.var(), sigma_2, atol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b086f070cd678a1132b91bb2453a0f1",
     "grade": false,
     "grade_id": "cell-2ddf5ef013e5ff82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's use numpy to generate samples of gaussian random variables\n",
    "n_dim, m = 1, 100000\n",
    "mean1, std1 = 0, 1.\n",
    "mean2, std2 = 3, 10.\n",
    "mean3, std3 = -7.5, 5.\n",
    "mean4, std4 = 0, 2.\n",
    "v1 = np.random.normal(loc=mean1,  scale=std1, size=(m, n_dim))\n",
    "v2 = np.random.normal(loc=mean2,  scale=std2, size=(m, n_dim))\n",
    "v3 = np.random.normal(loc=mean3,  scale=std3, size=(m, n_dim))\n",
    "v4 = np.random.normal(loc=mean4,  scale=std4, size=(m, n_dim)) \n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.hist(v1,density=True,bins=100,color='blue',alpha=0.5, label=f'Mean {mean1}, std {std1}')\n",
    "plt.hist(v2,density=True,bins=100,color='orange',alpha=0.5, label=f'Mean {mean2}, std {std2}')\n",
    "plt.hist(v3,density=True,bins=100,color='green',alpha=0.5, label=f'Mean {mean3}, std {std3}')\n",
    "plt.hist(v4,density=True,bins=100,color='red',alpha=0.5, label=f'Mean {mean4}, std {std4}')\n",
    "plt.xlim(-30,30), plt.legend(fontsize=20)\n",
    "plt.show()\n",
    "\n",
    "# Better code for same plot in the next cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef7ef6d0db727c792c0845b20e586e59",
     "grade": false,
     "grade_id": "cell-e0dc1cedbc209733",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 2 (read-only)\n",
    "# Inspect what is inside 'g_variables', zip(means, stds) and appreciate the code (and possible typos) you saved using\n",
    "# single line for loops (a.k.a. list comprehensions)\n",
    "means, stds = [0, 3, -7.5, 0], [1., 10., 5., 2.]\n",
    "g_variables = [np.random.normal(loc=mean, scale=std, size=(m, n_dim)) for mean, std in zip(means, stds)]\n",
    "# Create figure\n",
    "plt.figure(figsize=(15,10))\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "[plt.hist(v,density=True,bins=100,color=c,alpha=0.5, label=f'Mean {m}, std {std}') for m, std, v, c in zip(means, stds, g_variables, colors)]\n",
    "plt.xlim(-30,30), plt.legend(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20959e02d4b10a27d4e3b81833aedb75",
     "grade": false,
     "grade_id": "cell-a07fafde6da9ffe0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For now you have seen the *go to* strategy for generating gaussian scalar random variables! Can you generate in a similar way vector valued gaussian random variables?\n",
    "\n",
    "### How to generate gaussian random vectors of given mean and covariance? \n",
    "Say you are given a mean vector $\\mu_{new}$ and a covariance matrix $\\Sigma_{new}$ (positive semi-definite), how to generate a gaussian random vector with the proper mean and covariance? \n",
    "\n",
    "Consider a gaussian random vector $X \\in \\mathbb{R}^n$ (note that we are using the same notation, capital letters, to denote random variables and vectors of random variables). Once again mean and covariance fully describe the gaussian random vector, from now on let $X \\sim \\mathcal{N}(\\mu, \\Sigma)$.\n",
    "Remember also in this case an affine transformation of a gaussian random vector is a gaussian random vector. \n",
    "\n",
    "Therefore the same reasoning we made earlier is applicable here! \n",
    "\n",
    "Let's compute mean and covariance of a gaussian random vector after the application of an affine transformation.\n",
    "\n",
    "__Remark__: The affine transformation now is parametrized by the matrix $A \\in \\mathbb{R}^{n\\times n}$ and the vector $b \\in \\mathbb{R}^n$: $Y = AX+b$.\n",
    "\n",
    "From the theory we know $Y$ is a gaussian random variable. It is straightforward to compute its mean (using linearity of the expected value):\n",
    "\n",
    "$$\\mathbb{E}[Y] = A\\mathbb{E}[X] + b \\qquad \\text{(exactly as in the scalar case)}.$$\n",
    "\n",
    "Now recall the definition of covariance for a vector-valued random variable: $Cov[Y, Y] = \\mathbb{E}[(Y-\\mathbb{E}[Y])(Y-\\mathbb{E}[Y])^T]$. Pay attention to what is happening here: we are multiplying $(Y-\\mathbb{E}[Y])\\in \\mathbb{R}^{n\\times 1}$ (column vector) and $(Y-\\mathbb{E}[Y])\\in \\mathbb{R}^{1\\times n}$ (row vector), so the result is a $n\\times n$ matrix. That is why the covariance $Cov[Y,Y]$ is a $n\\times n$ matrix.\n",
    "\n",
    "Going back to our example:\n",
    "$$Cov[Y, Y] = \\mathbb{E}[(Y-\\mathbb{E}[Y])(Y-\\mathbb{E}[Y])^T] = \\mathbb{E}[(AX+b-A\\mathbb{E}[X]-b)(AX+b-A\\mathbb{E}[X]-b)^T] = \\\\ = A \\mathbb{E}[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^T]A^T = A Cov[X,X]A^T \\in \\mathbb{R}^{n\\times n}$$\n",
    "\n",
    "Putting everything together, we have: $Y\\sim \\mathcal{N}(A\\mu +b, A\\Sigma A^T)$.\n",
    "\n",
    "But we still do not know how to generate a gaussian random vector of given mean ($\\mu_{new}$) and covariance ($\\Sigma_{new}$)!\n",
    "\n",
    "For the sake of simplicity let's assume we start from a standard gaussian random vector $Z\\sim \\mathcal{N}(\n",
    "\\mathbf{0},I)$, where $\\mathbf{0}$ is a vector of dimension $n$ and $I \\in \\mathbb{R}^{n\\times n}$ is the identity matrix.\n",
    "If we apply the affine transformation $Y_{new} = AZ+b$ we get a gaussian with the following distribution $Y_{new} \\sim \\mathcal{N}(b, A I A^T)$ (where clearly $A I A^T = AA^T$). \n",
    "\n",
    "Once again we only need to impose the following equations to hold in order to find the proper affine transformation: \n",
    "$\\mu_{new} = b$ and $\\Sigma_{new} = AA^T$. The first is trivial while the second one might be a little more complicated (we are asking for a matrix A such that the moltiplication by its transpose gives exactly $\\Sigma_{new}$). \n",
    "Luckily for us we can consider $A$ as the \"square root\" of $\\Sigma$ and find the proper factorization through a very simple linear algebra technique: the spectral theorem (eigenvalue decomposition).\n",
    "\n",
    "In the case of a real symmetric $n\\times n$ matrix, $\\Sigma=\\Sigma^T$, we all know we can write the eigendecomposition as $\\Sigma = Q \\Lambda Q^T$ where $Q \\in \\mathbb{R}^{n \\times n}$ can be taken orthonormal (and real) and $\\Lambda$ is real. \n",
    "But we know $\\Sigma$ has another property: it is positive semi-definite! This implies that its eigenvalues are grater or equals to zero ($\\Lambda \\geq 0$).\n",
    "\n",
    "These are all the ingredients we need to solve $\\Sigma = AA^T$. Indeed if we write the eigenvalue decomposition in an equivalent way we can readily find the proper factorization: $\\Sigma= Q\\Lambda Q^T = Q \\sqrt{\\Lambda}\\sqrt{\\Lambda}Q^T$, where $\\sqrt{\\Lambda}$ is nothing but the diagonal matrix containing the square root of the $\\Lambda$ matrix entries (we can take the square root only thanks to the fact $\\Lambda \\geq 0$ !). \n",
    "\n",
    "If we now define $A=Q\\sqrt\\Lambda$ we get $\\Sigma = Q\\sqrt\\Lambda (Q\\sqrt\\Lambda)^T$ and we have solved the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6c907468f59d5e0945e5329108cda2a",
     "grade": false,
     "grade_id": "cell-d423426123eb2ef0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We choose to use row vectors in place of column vectors (to make you invariant to the notation)\n",
    "n_dim, m = 2, 100000\n",
    "\n",
    "# Mean and covariance matrix\n",
    "mu = np.array([1, 2])\n",
    "Sigma = np.array([[2, 1],\n",
    "                  [1, 2]])\n",
    "# Is Sigma a proper covariance matrix? p.s.d\n",
    "assert np.array_equal(Sigma, Sigma.T) # symmetric\n",
    "Lambda, Q = np.linalg.eig(Sigma)      # Have a look at the documentation to see what this function returns\n",
    "assert np.all(Lambda >= 0)            # positive semi-definite\n",
    "\n",
    "# Generate samples from a standard multivariate normal gaussian variable\n",
    "n = np.random.multivariate_normal(np.zeros(n_dim), np.eye(n_dim), size=(m))\n",
    "\n",
    "# Creating the factorization\n",
    "A = np.matmul(Q, np.diag(np.sqrt(Lambda)))\n",
    "# Applying the affine transformation\n",
    "x = mu + np.matmul(n, A.T)\n",
    "\n",
    "# If you are interested, many different factorizations exist. \n",
    "# Compare the realizations with the ones computed before: are they identical (w.r.t. the eigenvalues factorization)? \n",
    "# L = np.linalg.cholesky(Sigma) # See documentation, Sigma = L L^T\n",
    "# x = mu + np.matmul(n, L.T)\n",
    "\n",
    "# What about the empirical average and empirical covariance?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9426c6347cf6bd6d0e2ac89c35e10399",
     "grade": false,
     "grade_id": "cell-5dd40206d4d26f94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's compute the empirical average and empirical covariance for the random gaussian vector \n",
    "print(f\"The sample Mean is \\n {x.mean(axis=0)}\")\n",
    "x_centered = x - mu\n",
    "sample_cov = np.matmul(x_centered.T, x_centered)/m\n",
    "print(f\"The sample Covariance is \\n {sample_cov}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dccaab439a634cf8d9347fb7f5dc115",
     "grade": false,
     "grade_id": "cell-607a8564c7257906",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 3: Try the same as two cells above using gaussian vectors represented as column vectors. \n",
    "# Keep all the realizations stored in a numpy array with the following shape: (2, m). This means each random \n",
    "# vector is represented as a column random vector and we generate m of them.\n",
    "\n",
    "# Call the new generated data 'x_new'\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError() # Remove this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e7dd92447cb01dd4da767df584b1ca1",
     "grade": true,
     "grade_id": "cell-f4cd2474649c67f2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert x_new.shape == (n_dim, m)\n",
    "x_centered_new = x_new - mu.reshape(-1,1)\n",
    "sample_cov_new = np.matmul(x_centered_new, x_centered_new.T)/m\n",
    "assert np.isclose(np.abs(sample_cov_new-sample_cov).sum(), 0, atol=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d51887209c06ba3dd921631a3858a7a5",
     "grade": false,
     "grade_id": "cell-c95a8855b907bf3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 4\n",
    "# Create a function to check if a matrix is symmetric and p.s.d.\n",
    "def check_psd(A):\n",
    "    ''' Function to check whether a matrix is psd or not\n",
    "    :param A: n \\times n matrix stored as a numpy array\n",
    "    :return is_psd: Boolean representing whether the matrix is or is not a psd matrix\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() # Remove this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb0880d48122d2f04772ea038e065073",
     "grade": true,
     "grade_id": "cell-16b94d25305efc80",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert check_psd(Sigma)\n",
    "x_centered_new = x_new - mu.reshape(-1,1)\n",
    "Sigma_hat = np.matmul(x_centered_new, x_centered_new.T)/m\n",
    "assert np.isclose(np.abs(Sigma_hat - Sigma).mean(), 0., atol=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "013291196fb86b9ab8e9552c1b1116de",
     "grade": false,
     "grade_id": "cell-e364d74750c370ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we will visualize the two dimensional gaussian random variable we have generated using seaborn\n",
    "import seaborn as sns; sns.set()\n",
    "sns.jointplot(x=x[:,0],\n",
    "              y=x[:,1], \n",
    "              kind=\"kde\", # stands for kernel density estimate\n",
    "              space=0)\n",
    "\n",
    "# We will also visualize the true pdf of the two dimensional gaussian random variable\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "X = np.linspace(-3, 5, 100)\n",
    "Y = np.linspace(-2, 6, 100)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "pos = np.dstack((X, Y))\n",
    "true_pdf = multivariate_normal(mean=np.array([1, 2]), cov=np.array([[2, 1], [1, 2]])).pdf(pos)\n",
    "fig, ax = plt.subplots(2, 2, gridspec_kw={'width_ratios': [5, 1], 'height_ratios': [1, 5]}, figsize=(8, 8))\n",
    "ax_joint = ax[1, 0]\n",
    "contour = ax_joint.contour(X, Y, true_pdf, cmap=\"Blues\")\n",
    "ax_joint.set_xlabel(\"X\")\n",
    "ax_joint.set_ylabel(\"Y\")\n",
    "ax_joint.set_title(\"True pdf\")\n",
    "# X-axis marginal\n",
    "ax_marg_x = ax[0, 0]\n",
    "ax_marg_x.plot(X, norm.pdf(X, loc=0, scale=1), color=\"blue\")\n",
    "ax_marg_x.set_title(\"True marginal of X\")\n",
    "ax_marg_x.set_xlim(-3, 3)\n",
    "ax_marg_x.axis(\"off\")\n",
    "# Y-axis marginal\n",
    "ax_marg_y = ax[1, 1]\n",
    "ax_marg_y.plot(norm.pdf(Y, loc=0, scale=1), Y, color=\"blue\")\n",
    "ax_marg_y.set_title(\"True marginal of Y\")\n",
    "ax_marg_y.set_ylim(-3, 3)\n",
    "ax_marg_y.axis(\"off\")\n",
    "ax[0, 1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f6d894f80cc2d106e37b2f53fa3c837",
     "grade": false,
     "grade_id": "cell-869ea0a0e8a104d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's show scatter plot with directions given by the covariance matrix\n",
    "U, S, V_transpose = np.linalg.svd(Sigma)   # have a look at the documentation (you will need it in future labs)\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.scatter(x[:,0], x[:,1])\n",
    "\n",
    "def plot_line(direction, center, x_lims, color, label):\n",
    "    x = np.linspace(x_lims[0], x_lims[1], 1000)\n",
    "    if direction[0] != 0:\n",
    "        y = direction[1] / direction[0] * x\n",
    "    else:\n",
    "        y = np.linspace(x_lims[0], x_lims[1], 1000)\n",
    "        x = np.zeros_like(y)\n",
    "    plt.plot(x + center[0], y + center[1], color=color, label=label)\n",
    "\n",
    "max_value = np.abs(x).max(axis=0)[0]\n",
    "plot_line(U[:,0], mu, [-max_value, max_value], color='red', label='First axis')\n",
    "plot_line(U[:,1], mu, [-max_value, max_value], color='green', label='Second axis')\n",
    "plt.axis('equal'), plt.legend(fontsize=20)\n",
    "# Each axis std is given np.sqrt(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1c3a97e7e0e9009b59ecd2f5599b0a2",
     "grade": false,
     "grade_id": "cell-a15938caa0b658bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 5 (answer in the following cell)\n",
    "# Have a look at the documentation of svd.\n",
    "# Compare U and V, what do you observe? Try to get Sigma back from U,S,V\n",
    "U, S, V_transpose = np.linalg.svd(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e52e3b57bbc72b6156e5969da4c5bed1",
     "grade": false,
     "grade_id": "cell-685e6f0d4fb30211",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError() # Remove this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e39e3a0d8fab98f2fbce6467343fc24",
     "grade": false,
     "grade_id": "cell-6e34c875f89258fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 6 (answer in the following cell)\n",
    "# What if you do not know the true covariance matrix? Can you estimate its spectrum and eigenvectors using data?\n",
    "U_x, S_x, V_x_transpose = np.linalg.svd(x_centered / np.sqrt(m), full_matrices=False)\n",
    "# Compare U_x with U and V_x with V, what do you observe? \n",
    "# Can you build the sample covariance matrix only from S_x and V_x? How?\n",
    "# Is this a smart way to compute the sample covariance? If not, why (try with more data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb405ef312278794a886162e87db1c64",
     "grade": false,
     "grade_id": "cell-f19f39900e512b86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Recall that the empirical covariance matrix is $\\hat{\\Sigma} = \\frac{(X - \\mu)^T}{\\sqrt{m}} \\frac{(X - \\mu)}{\\sqrt{m}}$, where $X \\in \\mathbb{R}^{m \\times d}$, and that $\\frac{(X - \\mu)}{\\sqrt{m}} = U_x S_x V_x^T$.\n",
    "\n",
    "Therefore\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\hat{\\Sigma} &= \\frac{(X - \\mu)^T}{\\sqrt{m}} \\frac{(X - \\mu)}{\\sqrt{m}} \\\\\n",
    "             &= (U_x S_x V_x^T)^T (U_x S_x V_x^T) \\\\\n",
    "             &= V_x S_x^2 V_x^T\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "This is *not* a smart way to compute the sample covariance, as we are computing the SVD of a large matrix $\\in \\mathbb{R}^{m \\times d}$, which is computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd376c5255a5e585626fd8071501b9e1",
     "grade": true,
     "grade_id": "cell-64a84f540ec6da80",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError() # Remove this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b52f139871307f432fcd4b83bc2ec922",
     "grade": false,
     "grade_id": "cell-c2a070150d90d64d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# True vs empirical risk\n",
    "\n",
    "In this section we are going to compare the true risk and the empirical risk in the most simple scenario you can imagine: scalar Least Squares using a linear model class.\n",
    "\n",
    "__Note__: Since we want to make things as clear as possible, we are going to be very pedantic and emphasise the difference between a random variable and its realizations.\n",
    "We are going to use the underbar to denote a random variable. Therefore the notation is as follows: $\\underline y$ is a random variable (say a Gaussian, a Bernoulli, Chi-square, ...) and $y$ is its realization, which is a number (say 1.2, 10.0, 99.032, ...).\n",
    "\n",
    "To begin with, we will setup the notation and the main assumptions we need in order to perform all the necessary computations. We are going to compute integrals which might not have closed form solutions! That is why we need some assumptions.\n",
    "\n",
    "We are going to compute the integral to get the true risk for a simple model class, the scalar linear models:\n",
    "$$\\mathcal{H}_{lin} := \\{x \\mapsto wx : w\\in \\mathbb{R}\\}$$\n",
    "in the case of squared loss.\n",
    "\n",
    "We are interested in comparing the following quantities:\n",
    "\n",
    "$$L_\\mathcal{D}(h) := \\mathbb{E}_\\mathcal{D}[l(\\underline y, h(\\underline x))] := \\int l( y, h( x)) p( x,  y) dx dy \\quad \\longrightarrow \\quad \\text{this is a number}$$ \n",
    "\n",
    "$$L_\\mathcal{S}(h) := \\frac{1}{m} \\sum_{i=1}^{m} l(y_i, h(x_i)) \\quad \\longrightarrow \\quad \\text{this is a number: a realization of the following random variable}$$\n",
    "\n",
    "$$\\underline{L_\\mathcal{S}(h)}:= \\frac{1}{m} \\sum_{i=1}^{m} l(\\underline y_i, h(\\underline x_i)) \\quad \\longrightarrow \\quad \\text{this is a random variable}$$\n",
    "\n",
    "where $l$ is a loss funtion (a deterministic function), $h \\in \\mathcal{H}$ and $\\mathcal D$ is the joint distribution of $\\underline y$ and $\\underline x$ (outputs and inputs to our models). \n",
    "\n",
    "In the following exercise we will need to define what we mean by each ones of these quantities.\n",
    "\n",
    "1. Let's start by defining what is the data generative model $\\mathcal D$: you can think of it as the object Dice in the previous lab (it describes the random variables which are generating all the data we see). We need to specify the joint distribution of $\\underline x$ and $\\underline y$: $p(x, y)$. Remember we can write the joint probability of two general random variables using conditional probabilities, so that we have: $p(x, y)=p(y|x)p(x)$. Don't forget $p(y|x)$ describes a random variable on $\\underline y$, while $x$ is to be considered as a fixed number (you can think of it as an outcome)! Up to now everything is super general, so let's introduce some assumption in order to make our lives easier. We will assume $\\underline x\\sim \\mathcal{N}(\\mu_x, \\sigma_x^2)$, and $p( y| x)\\sim \\mathcal{N}(a x, \\sigma^2)$. Note here the mean of $\\underline y$ depends on the outcomes $x$. How to generate samples from $\\underline y$? Take $\\underline x$, sample it, get a number, say $ x=1.234$, and now generate a sample from a gaussian random variable whose mean is $a * 1.234$ and whose variance is $\\sigma^2$.\n",
    "\n",
    "Why did I choose such a strange $p( x, y)$? You can prove such a joint distribution is derived from the following very simple model: \n",
    "$$\\underline y := a \\underline x + \\underline e$$\n",
    "where $\\underline e \\sim \\mathcal N (0, \\sigma^2)$ and $\\underline x$ is independent from $\\underline e$.\n",
    "\n",
    "\n",
    "\n",
    "__TODO__: Do the computations and show the distribution of $\\underline y$ is $\\mathcal N (a\\mu_x, a^2 \\sigma^2_x + \\sigma^2)$.\n",
    "\n",
    "2. We will adopt the squared loss, i.e. $l(b, c) := (b-c)^2$.\n",
    "\n",
    "3. We will choose our model class to be the set of scalar linear models, $\\mathcal{H}_{lin} := \\{x \\mapsto wx : w\\in \\mathbb{R}\\}$\n",
    "\n",
    "\n",
    "__True risk computation__: In this very simple case we can actually compute the true risk by hand (solving the integral in the definition). If you want you can compute the integral using the densities... but I prefer to save some time and use the properties of the expected value. Therefore, replacing the definition of $l(b, c) := (b-c)^2$ and $\\underline y$ inside the one of the true risk, recalling the basic relation $var[\\underline y] = \\mathbb{E}[\\underline y^2] - \\mathbb{E}[\\underline y]^2$ and using the fact that $\\underline x$ and $\\underline e$ are independent (and therefore $\\mathbb{E}[\\underline x \\underline e] = \\mathbb{E}[\\underline x] \\mathbb{E}[\\underline e] = 0$) we get:\n",
    "\n",
    "\\begin{align}\n",
    "L_\\mathcal D (h) &= \\mathbb{E}_{\\underline x, \\underline y}[(\\underline y - h(\\underline x))^2] = \\mathbb{E}_{\\underline x, \\underline e}[(a\\underline x + \\underline e - w\\underline x)^2] \\\\ \n",
    "&= \\mathbb{E}_{\\underline x, \\underline e}[((a-w)\\underline x + \\underline e)^2] = \\mathbb{E}_{\\underline x, \\underline e}[(a-w)^2\\underline x^2 + 2 \\underline x \\underline e + \\underline e^2 ] \\\\\n",
    "&= (a-w)^2\\mathbb{E}_{\\underline x}[\\underline x^2]+\\mathbb{E}_{\\underline e}[\\underline e^2] = (a-w)^2(var[\\underline x]+\\mathbb{E}_{\\underline x}[\\underline x]^2)+(var[\\underline e]+\\mathbb{E}_{\\underline e}[\\underline e]^2) \\\\\n",
    "&=(a - w)^2(\\sigma_x^2 + \\mu_x^2)+\\sigma^2\\\\\n",
    "\\end{align}\n",
    "\n",
    "Boring, but easy! \n",
    "\n",
    "In the following you will implement a function to compute $L_\\mathcal D (h)$ (I am using the word \"computing\" and not \"approximating\" here, so you should use the formula we just derived!) and a simple class to represent the joint probability of $\\underline x$ and $\\underline y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e226f93a8756a45dd550ac08b905f9d",
     "grade": false,
     "grade_id": "cell-842597daf5802441",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 7: Write the class method generate_samples in order to get samples from the joint distribution (x,y)\n",
    "class D():\n",
    "    def __init__(self, mean_x, var_x, var_e, a):\n",
    "        self.mean_x, self.var_x = mean_x, var_x\n",
    "        self.var_e = var_e\n",
    "        self.a = a\n",
    "        \n",
    "    def generate_samples(self, m : int) -> np.ndarray:  # -> specifies the type of the output but is ignored by the compiler\n",
    "        '''\n",
    "        Function to generate outcomes of the joint distribution p(x,y) described in the cell above\n",
    "        \n",
    "        :param m: Number of measurements\n",
    "        :returns: a tuple containing both outcomes from x and from y \n",
    "            WHERE:\n",
    "            :x: is a numpy vector of dimension (1, m) containing the outcomes of x \n",
    "            :measurements: is a numpy vector of dimension (1, m) containing the outcomes of y\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError() # Remove this line\n",
    "        return x, measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94e51600e8f51d72866193e724c1a0dd",
     "grade": false,
     "grade_id": "cell-4473f779ea2bc30f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mean_x, var_x, var_e, a = 3, 2, 1, 5\n",
    "joint_distr = D(mean_x=mean_x, var_x=var_x, var_e=var_e, a=a)\n",
    "m = 100000\n",
    "measurements = joint_distr.generate_samples(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0718428d41e2f1ad42a572bd3b858a04",
     "grade": true,
     "grade_id": "cell-4b3c4b2450a84b02",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert measurements[0].shape == (1, m)\n",
    "assert measurements[1].shape == (1, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0ebfeed61adc892f125b7a5d0078c7b",
     "grade": false,
     "grade_id": "cell-0220456fba322864",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 8: write a function to compute the true risk (using the formula we derived in the cell above!)\n",
    "def compute_true_risk(a, w, mu_x, sigma_x_2, sigma_2):\n",
    "    '''\n",
    "    This function assumes all its inputs but w are scalars, in the case w is a vector this function should be \n",
    "    implemented such that it returns a vector of true risks for each value in the array w. \n",
    "    SUGGESTION: Try to implement this function as all its inputs (w too) were scalars, then try to feed a numpy vector \n",
    "    and see what happens (this is the magic of Numpy!)\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() # Remove this line\n",
    "    return true_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "687a86d840b34c8ea559c209f0a39641",
     "grade": true,
     "grade_id": "cell-1eef479e06977caf",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert compute_true_risk(a, a, mean_x, var_x, var_e) == var_e\n",
    "assert compute_true_risk(a, 0.123, mean_x, var_x, var_e) == 262.636419\n",
    "w_vector = np.linspace(-5, 15, 1000)\n",
    "assert compute_true_risk(a, w_vector, mean_x, var_x, var_e).shape == w_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ae2161c3758df392e1ad6760cdf0699",
     "grade": false,
     "grade_id": "cell-f2315e9a85353dea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's plot the true risk\n",
    "w_possible = np.linspace(-5, 15, 1000)\n",
    "true_risk = compute_true_risk(a, w_possible, mean_x, var_x, var_e)\n",
    "plt.plot(w_possible, true_risk)\n",
    "plt.ylabel('True risk')\n",
    "plt.xlabel('w parameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4016f60ad941385e99dab209a2cddb6a",
     "grade": false,
     "grade_id": "cell-e2603a89eb1f57da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "__TODO 9__:\n",
    "\n",
    "What do you observe? What is the shape you get? Why? Which is the best parameter you could possibly choose? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b85437e52760b1869b9700f5a8dbff5d",
     "grade": true,
     "grade_id": "cell-0e88427d5f67d050",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e1b757c139352c576f009e465e8dbd5",
     "grade": false,
     "grade_id": "cell-5f8320ff05451340",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Least Squares\n",
    "\n",
    "By definition, the Least Squares method finds the optimal parameter values $w$ for a given model class by minimizing the sum of squared residuals $R$, given by:\n",
    "\n",
    "$$R(w) = \\sum_{i=1}^m r_i^2, \\qquad \\text{where} \\quad r_i(w) = y_i - h_w(x_i)$$\n",
    "\n",
    "Note that under the assumption of a linear model class, the residuals above can be specified as $r_i(w) = y_i - wx_i$. We can easily see that under the assumption of the squared loss, we have $r_i^2(w) = l(y_i, h_w(x_i))$, so that:\n",
    "$$ \\frac{1}{m}R(w) = L_\\mathcal{S}(h_w) $$\n",
    "\n",
    "Therefore the Least Squares method is actually equivalent to the empirical risk minimization with respect to the given data:\n",
    "\n",
    "$$\\begin{align}\n",
    "    w_{LS} &= min_w[L_\\mathcal{S}(h_w)] \\\\\n",
    "    &= min_w \\left\\{ \\frac{1}{m} \\sum_{i=1}^m(y_i - wx_i)^2 \\right\\}\n",
    "\\end{align}$$\n",
    "\n",
    "The stationary points of the objective can be found by imposing the derivative of the objective equal to zero:\n",
    "\n",
    "$$\\frac{\\partial L_\\mathcal{S}(h_w)}{\\partial w} = 0 \\\\ \\implies \\frac{\\partial}{\\partial w} \\left[ \\frac{1}{m} \\sum_{i=1}^m(y_i - wx_i)^2 \\right] =0$$\n",
    "\n",
    "You should know ho to go on from this point onward!\n",
    "\n",
    "*Hint: For the scalar case the solution is trivial. You can start from there. Of course the vectorial case is more general!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22edb90604f300f088ab576cc726336a",
     "grade": false,
     "grade_id": "cell-1efede4ccf42274f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 10:\n",
    "# Now write a simple function to compute the empirical loss of given a parameter w and the m scalar realizations x and y\n",
    "def empirical_training_loss(w, x, y) -> np.ndarray:\n",
    "    '''\n",
    "    Function to compute the empirical squared loss (if you give more than one scalar parameters, say p, it will compute the\n",
    "    empirical loss for each scalar parameter).\n",
    "    :param w: a scalar or numpy array of shape (p, 1)\n",
    "    :param x: a numpy vector of shape (1, m) representing the input x \n",
    "    :param y: a numpy vector of shape (1, m) representing the output y \n",
    "    \n",
    "    :returns: numpy array. of shape (p, 1) containing the empirical loss computed for each scalar parameter in the \n",
    "              vector w \n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() # Remove this line\n",
    "    return empirical_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c7411e4dc3d413e93efb1a7161f6765",
     "grade": true,
     "grade_id": "cell-da803ef057eef486",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(empirical_training_loss(a, measurements[0], measurements[1])[0]) == np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7662264040d767e71af768e49bfcb32",
     "grade": false,
     "grade_id": "cell-e6d6445dcb3fbab0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "measurements = joint_distr.generate_samples(10)\n",
    "empirical_loss = empirical_training_loss(w_possible.reshape(-1,1), measurements[0], measurements[1])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(w_possible, true_risk, label='true risk')\n",
    "axes[0].plot(w_possible, empirical_loss, label='empirical risk')\n",
    "axes[0].set_xlabel('w parameter')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(w_possible, true_risk, label='true risk')\n",
    "axes[1].plot(w_possible, empirical_loss, label='empirical risk')\n",
    "axes[1].set_xlim(4,6)\n",
    "axes[1].set_ylim(0,15)\n",
    "axes[1].set_xlabel('w parameter (zoomed)')\n",
    "axes[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1afe4509ea420127bf384089435bf9a",
     "grade": false,
     "grade_id": "cell-91af99906234f95d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 11: Write a function to compute the LS solution to a scalar LS problem. \n",
    "def compute_ls_solution(y, x) -> np.float64:\n",
    "    '''\n",
    "    Function that implements scalar LS solution given data x (inputs) and y (outputs)\n",
    "    \n",
    "    :param x: numpy vector of shape (1, m) representing inputs x\n",
    "    :param y: numpy vector of shape (1, m) representing outputs y\n",
    "    \n",
    "    :returns: scalar parameter w_hat solution to the ERM problem\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() # Remove this line\n",
    "    return w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44d92bb1112346577398e8b655d272fa",
     "grade": true,
     "grade_id": "cell-022be3a6ece1df56",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(compute_ls_solution(measurements[1], measurements[0])) == np.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da9732f85a2d49b24ddf141c7498122f",
     "grade": true,
     "grade_id": "cell-d5054f6e7a2e163e",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 12: Complete the following code. We are generating different measurements from our joint distribution \n",
    "# and comparing them to the true risk. Moreover for each empirical loss realization we compute its minimizer.\n",
    "# We use an histogram to see the distribution of optimizers we can achieve when we sample a different training sets.\n",
    "# Question: play with m to see how the empirical loss changes and how the corresponding w_opt changes. What do you \n",
    "# observe?\n",
    "\n",
    "T_estimates = 1000\n",
    "m = 10\n",
    "w_opt_realizations = []\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(15,7))\n",
    "axes[0].plot(w_possible, true_risk, linewidth=5, color='r', label='true risk')\n",
    "\n",
    "for t in range(T_estimates): \n",
    "    # Generate the training dataset, composed by realizations from the joint distribution of x and y\n",
    "    x, y = None, None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() # Remove this line\n",
    "    \n",
    "    # Evaluate the empirical loss\n",
    "    empirical_loss = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() # Remove this line\n",
    "    \n",
    "    # Get the optimal ERM w_opt\n",
    "    w_opt = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() # Remove this line\n",
    "    \n",
    "    w_opt_realizations.append(w_opt)\n",
    "    if t < 15: # To avoid clutter in the plot\n",
    "        axes[0].plot(w_possible, empirical_loss)\n",
    "\n",
    "axes[0].set_xlim([4,6])\n",
    "axes[0].set_ylim([0,15])  \n",
    "axes[0].legend()\n",
    "\n",
    "bins = np.linspace(2, 10, 500)\n",
    "axes[1].hist(w_opt_realizations, bins, density=True)\n",
    "axes[1].plot(w_possible, true_risk)\n",
    "axes[1].set_xlim([4,6])\n",
    "axes[1].set_ylim([0,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "591f0e51eb027f20d0d15ba73f8d47ac",
     "grade": false,
     "grade_id": "cell-6527ab9c479cf6eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 13 (read-only): We now will increase the number of data in the training dataset and see what happens to the empirical loss.\n",
    "# What do you observe?\n",
    "\n",
    "Ns = [3, 10, 50, 1000, 10000]\n",
    "\n",
    "fig, axes = plt.subplots(1,1,figsize=(15,7))\n",
    "axes.plot(w_possible, true_risk, linewidth=5, color='r')\n",
    "x, y = joint_distr.generate_samples(Ns[-1])\n",
    "for m_t in Ns:  \n",
    "    x_train, y_train = x[0, :m_t], y[0, :m_t]\n",
    "    empirical_loss = empirical_training_loss(w_possible.reshape(-1,1), x_train, y_train)\n",
    "    w_opt_realizations.append(compute_ls_solution(y_train, x_train))\n",
    "    axes.plot(w_possible, empirical_loss, label=f'm_t = {m_t}')\n",
    "\n",
    "axes.set_xlim([4,6])\n",
    "axes.set_ylim([0,15])\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e18bc5640a95c8521f38582fd41cb267",
     "grade": false,
     "grade_id": "cell-80c4ab6bd93dc24f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 14: Write a function to compute the optimal LS solution in vector notations. Assume X^TX is invertible \n",
    "def vector_LS_solution(X : np.ndarray, Y : np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Assuming X^TX is invertible this functions returns the optimal parameters of a LS problem for which the parameters\n",
    "    are vectors.\n",
    "    :parameters X: input data matrix of shape (m, p)\n",
    "    :parameters Y: output data array of shape (m, 1)\n",
    "    \n",
    "    :returns: optimal w_opt solution of the LS problem (its shape is (p, 1))\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError() # Remove this line\n",
    "    return w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22aa3392403b0a1baf74880a059112f4",
     "grade": false,
     "grade_id": "cell-906187b3faf7a3bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.random.normal(size=(10000, 2))\n",
    "A = np.array([[3],[1]]).reshape(-1,1)\n",
    "E = np.random.normal(size=(10000, 1))\n",
    "Y = np.matmul(X, A) + E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddcf34f582c06f82e16c36fb6f01af0a",
     "grade": true,
     "grade_id": "cell-da0a126604220cbe",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert vector_LS_solution(X, Y).shape == (2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9a8d2d8598b54d5ba58bf586a82c131",
     "grade": false,
     "grade_id": "cell-183b69d2c99233f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# What do you observe? Why?\n",
    "vector_LS_solution(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
